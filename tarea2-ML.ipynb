{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2 - Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claudia Hazard 201404523-9\n",
    "## Matías Araya 201173082-8\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tipos de fronteras en Clasificación\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Se comienza creando dataset con 2 dimensiones, conformado por dos conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples=500\n",
    "mean = (0,-4)\n",
    "C = np.array([[0.3, 0.1], [0.1, 1.5]])\n",
    "datos1 = np.random.multivariate_normal(mean, C, n_samples)\n",
    "outer_circ_x = np.cos(np.linspace(0, np.pi, n_samples))*3\n",
    "outer_circ_y = np.sin(np.linspace(0, np.pi, n_samples))*3\n",
    "datos2 = np.vstack((outer_circ_x,outer_circ_y)).T\n",
    "from sklearn.utils import check_random_state\n",
    "generator = check_random_state(10)\n",
    "datos2 += generator.normal(scale=0.3, size=datos2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrega ruido al conjunto de datos para así realizar un estudio mas realista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((datos1, datos2), axis=0)\n",
    "print len(X)\n",
    "n = 20 #ruido/noise\n",
    "y1 = np.zeros(datos1.shape[0]+n)\n",
    "y2 = np.ones(datos2.shape[0]-n)\n",
    "y = np.concatenate((y1,y2),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la configuración de el código anterior existen $1000$ datos en total, los cuales $520$ corresponden a un grupo (puntos azules) y $480$ a otro (puntos verdes). Se nota como el ruido de de $20$ correspondientes realmente al grupo azul, se asemejan más a la figura del grupo verde por lo que genera ruido a la muestra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `visualize_border` es de utilidad para visualizar el conjunto de datos con su respectivo clasificador, el que se utilizará en preguntas posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_border(model,x,y,title=\"\"):\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "    plt.scatter(x[:,0], x[:,1], s=50, c=y, cmap=plt.cm.winter)\n",
    "    h = .02 # step size in the mesh\n",
    "    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) LDA\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "model_lda = LDA()\n",
    "model_lda.fit(X,y)\n",
    "visualize_border(model_lda,X,y,\"LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Linear Discriminant Analysis (LDA), como se ve en la figura mostrada, traza una linea clara que logra separar la clasificación de ambos grupos, quedando así la mayoría de los puntos azules por un lado y la totalidad de los verdes en el otro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) QDA\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "model_qda = QDA()\n",
    "model_qda.fit(X,y)\n",
    "visualize_border(model_qda,X,y,\"QDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso con Quadratic Discriminant Analysis (QDA), como lo dice su nombre al ser de tipo cuadrático, logra crear una curva asemejandose de mejor manera a la figura y cualitativamente se podría decir que clasifica mejor que LDA. Esto sin embargo conlleva un mayor costo de computación además de mayor posibilidad de overfitting, con lo que se analiza en próxima sección si vale la pena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que los datos se encuentren distribuidos de forma normal es fundamental para el clasificador LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(datos1, bins='auto')\n",
    "plt.title(\"Histograma datos 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(datos2, bins='auto')\n",
    "plt.title(\"Histograma datos 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_lda = model_lda.predict(X)\n",
    "y_pred_qda = model_qda.predict(X)\n",
    "\n",
    "y_true = y\n",
    "\n",
    "print(\"Miss Classification Loss LDA: %f\"%(1-accuracy_score(y_true, y_pred_lda)))\n",
    "print(\"Miss Classification Loss QDA: %f\"%(1-accuracy_score(y_true, y_pred_qda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto se puede ver que la diferencia entre los errores es muy pequña, donde LDA tiene un error de clasificación de $0.021$ y QDA es levemente menor con $0.020$. Con esto se puede comprobar, para este pequeño caso, que no es necesario utilizar QDA ya que genera un gasto extra además de generar un mayor overfitting y no se obtiene una mejor calidad en la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "\n",
    "def visualize_border_interactive(param):\n",
    "    model = train_model(param)\n",
    "    visualize_border(model,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "def train_model(param):\n",
    "    model=LR() #define your model\n",
    "    model.set_params(C=param,penalty='l2')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 0.01\n",
    "p_max = 2\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar en el gráfico interactivo, cambiando el paramétro se mueve ligeramente la línea divisiora de la regresión que separa ambas clases. Esto se puede asemejar a lo que hace Ridge o Lasso, donde se penalizan los coeficientes restando importancia a los que influyen de menor manera en el modelo. Así, si el parámetro es bajo, cercano a cero, tiene menos aceptación a que haya puntos mal clasificados. Con el parámetro en $0.1$ ni un punto verde queda completamente en el grupo azul. Mientras que, con un parámetro más alto, por ejemplo $10$ se nota como cambia donde la pendiente de la recta se hace más pronunciada, aceptando así algunos puntos verdes en el grupo azul. En resumen, el parámetro $C$ corresponde a una medida de regularización. Entre más bajo sea, más fuerte es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM), a diferencia de la otras técnicas que se han visto, crea un hiperplano separando las 2 clases lo mas ampliamente posible. No como el resto de los métodos que minimiza los errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "\n",
    "def train_model(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='linear')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 0.1\n",
    "p_max = 1\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El efecto es casi el mismo que la Regresión Logística, donde con una valor alto de el parámetro $C$ la línea divisora queda con una mayor pendiente, aceptando así los valores azules correspondientes al ruido pero incluyendo también unos pocos del conjunto verde. Mientras que con un $C$ cercano a cero permite menos puntos azules de los correspondientes al ruido pero no acepta ni un verde en el conjunto azul.\n",
    "\n",
    "En general, el valor $C$ dice cuanto se quiere evitar de missclasification en cada conjunto de entrenamiento. Para valores grandes de $C$, la optimización eligirá un margen menor para el hiperplano. Por otro lado un valor bajo de $C$ el optimizador causará un margen mayor separando el hiperplano, a pesar de que genere un mayor missclasification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una solución simplificada para SVM no Lineal puede ser escrita como:\n",
    "\n",
    "$$\\hat{f(x)} = \\displaystyle\\sum_{i=1}^{n} \\hat{\\alpha_i}y_i K(x,x_i) + \\hat{\\beta_0}$$\n",
    "\n",
    "Donde $K(x, x_i)$ es la función Kernel y se cumple que para todo $x_i$, $0 < \\alpha_i < C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='rbf')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 0.01\n",
    "p_max = 1\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='poly')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 0.1\n",
    "p_max = 1\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rol del parámetro $C$ es claro en un espacio con más atributos, esto dado que una separación perfecta es usualmente conseguible. Un valor grande de $C$ hará que sea más ondulado, mientras que uno más pequeño reflejará curvas más suaves.\n",
    "\n",
    "El efecto anterior explicado se puede ver de buena manera con el Kernel rbf, donde cambiando el parámetro con un valor de $1$ quedan las líneas más onduladas encerrando al conjunto verde. Mientras cuando se asigna un valor cercano a cero, $0.1$, se ve reflejado que las línas encierran al grupo azul siendo más suave las curvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## h)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "def train_model(param):\n",
    "    model= Tree() #edit the train_model function\n",
    "    model.set_params(max_depth=param,criterion='gini',splitter='best')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 1\n",
    "p_max = 4\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro que se varía es el de maxima profundidad del árbol. Primero se realiza el ánalisis con el criterio Gini, el cual es la suma de las varianzas de cada distribución. Con el parámetro igual a $1$, solo se tendrá un árbol de profundidad 1, por lo que, como se ve en la imagen, separa el conjunto de datos en $2$. Separando claramente ambos conjuntos de datos (azules y verdes).\n",
    "\n",
    "Si esta parámetro aumenta, se agrega un nivel de decisión y de profundidad, por lo que se vuelve más preciso el árbol. Como se puede ver en el código anterior, con 3 niveles es capaz de capturar gran parte del ruido en el conjunto verde a su correspondiente conjunto.\n",
    "\n",
    "La gran ventaja de este método es su simpleza para ser interpretado, pero sufre de un gran overfitting comparado con los otros métodos anteriormente vistos. Esto se puede comprobar que si el parámetro es $4$, se agrega una tercera clase inexistente con puntos azules y verdes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "def train_model(param):\n",
    "    model= Tree() #edit the train_model function\n",
    "    model.set_params(max_depth=param,criterion='entropy',splitter='best')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 2\n",
    "p_max = 8\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cambia el criterio a information gain, este intenta minimizar la suma pesada de las entropías resultantes. Este es equivalente a maximizar la ganancia de información al dividir el nodo. En otras palabras se debe buscar una medida de la pureza de cada nodo resultante.\n",
    "\n",
    "Con poca entropía es claro saber a que clase corresponde, si es muy entrópica no se tiene esa claridad.\n",
    "\n",
    "Con este criterio es notorio que se obtienen mejores resultado cuando el parámetro es alto en comparación a Gini, donde por ejemplo con $8$ logra diferenciar los puntos azules del ruido sin agregar puntos verdes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def train_model(param):\n",
    "    model = KNeighborsClassifier()\n",
    "    model.set_params(n_neighbors=param)\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 1\n",
    "p_max = 10\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo kNN se refiere a k Nearest Neighbors, por lo que el parámetro $k$ es para ver los vecinos más cercanos. Si se elige por ejemplo $k=2$, un punto quedará en el conjunto donde sus dos vecinos más cercanos estén. Esto se puede ver cualitativamente en el gráfico, donde con un $k$ bajo en la sección donde está el ruido las líneas se dispersan de manera que encierran de a uno los puntos. Mientras que con un $k$ mayor la línea se hace más continua."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análisis de audios como datos brutos\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "def clean_filename(fname, string):\n",
    "    file_name = fname.split('/')[1]\n",
    "    if file_name[:2] == '__':\n",
    "        file_name = string + file_name\n",
    "    return file_name\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "def load_wav_file(name, path):\n",
    "    s, b = wavfile.read(path + name)\n",
    "    assert s == SAMPLE_RATE\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)\n",
    "Se crea el dataset a partir de  set_a.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./latidos/set_a.csv')\n",
    "df.info()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se calculan la cantidad de registros por clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "art=0\n",
    "mur=0\n",
    "extra=0\n",
    "norm=0\n",
    "nulo=0\n",
    "for i in df[\"label\"]:\n",
    "    if i==\"artifact\":\n",
    "        art+=1\n",
    "    elif i==\"murmur\":\n",
    "        mur+=1\n",
    "    elif i==\"extrahls\":\n",
    "        extra+=1\n",
    "    elif i==\"normal\":\n",
    "        norm+=1\n",
    "    else:\n",
    "        nulo+=1\n",
    "print (\"Tipo artifact:\"+str(art))\n",
    "print (\"Tipo extrahls:\"+str(extra))\n",
    "print (\"Tipo murmur:\"+str(mur))\n",
    "print (\"Tipo normal:\"+str(norm))\n",
    "print (\"Tipo no determinado:\"+str(nulo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset esta compuesto por 4 columnas de las cuales dataset y sublabel no aportan mucho pues dataset siempre toma el valor de a y sublabel no tiene ningun dato. En cuanto a las columnas fname y label, fname corresponde a el nombre del archivo que contiene la data, el tipo de sonido y luego el nombre del archivo wav, mientras que label corresponde al tipo de sonido nuevamente.\n",
    "\n",
    "Ademas hay 40 registros del tipo artifact, 19 del tipo extrahls, 34 murmur, 31 normal y 52 sin tipo especifico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "\n",
    "Se leen los archivos .wav y se transforman en secuencias de tiempo, es decir, la amplitud del audio espaciadas de manera uniforme en el tiempo. Luego se realiza un padding de ceros al final de cada secuencia para que todas las secuencias de tiempo queden con el mismo largo o misma cantidad de mediciones. Es importante realizar esto pues para realizar el aprendizaje de maquina mas adelante se realizará una transformada de fourier pasando las amplitudes a frecuencias, al realizar el zero padding todos los registros estarán en el mismo espectro de frecuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def padd_zeros(array,length):\n",
    "    aux = np.zeros(length)\n",
    "    aux[:array.shape[0]] = array\n",
    "    return aux\n",
    "new_df =pd.DataFrame({'file_name' : df['fname'].apply(clean_filename,string='Aunlabelledtest')})\n",
    "new_df['time_series'] = new_df['file_name'].apply(load_wav_file, path='./latidos/')\n",
    "new_df['len_series'] = new_df['time_series'].apply(len)\n",
    "new_df['time_series']=new_df['time_series'].apply(padd_zeros,length=max(new_df['len_series']))\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario realizar un padding para tener la misma cantidad de elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n",
    "Se cambian las etiquetas de los audios por otras asignadas por un doctor experto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_labels =[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2,\n",
    "2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
    "1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0,\n",
    "2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0,\n",
    "0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "labels = ['artifact','normal/extrahls', 'murmur']\n",
    "new_df['target'] = [labels[i] for i in new_labels]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "art=0\n",
    "mur=0\n",
    "norm=0\n",
    "\n",
    "for i in new_df[\"target\"]:\n",
    "    if i==\"artifact\":\n",
    "        art+=1\n",
    "    elif i==\"murmur\":\n",
    "        mur+=1\n",
    "        extra+=1\n",
    "    elif i==\"normal/extrahls\":\n",
    "        norm+=1\n",
    "\n",
    "print (\"Tipo artifact:\"+str(art))\n",
    "print (\"Tipo murmur:\"+str(mur))\n",
    "print (\"Tipo normal/extrahls:\"+str(norm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al realizar los cambios existen solo 3 clases. Artifact con 58 registros, murmur con 53 y normal/extrahls con 65. Al tener etiquetas mal asignadas con los datos se entrenará a la maquina erroneamente, es decir, la función creada por la maquina predecirá con muchos mas fallos la asignación para un dato de entrada.\n",
    "Si se cambiara un solo dato esta problematica dependerá de la forma en que se esté entrenando la maquina pues hay técnicas que son más y otras menos sensibles a outliers.\n",
    "Para la utilización de una regresión logística dependerá de la cantidad de datos pues si se tienen pocos datos y uno se cambia este influirá fuertemente en la estimación de los parametros, mientras que al ser un dataset muy grande y solo un \n",
    "outlier no influirá significativamente. En cambio al utilizar regresión lineal cualquier valor mal ingresado podría generar un gran corrimiento de la pendiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)\n",
    "Se codifican las clases con valores numericos, el valor 0 corresponde a artifact, 1 a murmur y 2 a normal/extrahls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df[\"target\"] = new_df[\"target\"].astype('category')\n",
    "cat_columns = new_df.select_dtypes(['category']).columns\n",
    "new_df[cat_columns] = new_df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "df_f=new_df\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)\n",
    "Se desordenan los datos para que no se encuentren ordenados por las etiquetas y se crea una matriz de amplitudes en el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = new_df.sample(frac=1,random_state=44)\n",
    "X = np.stack(new_df['time_series'].values, axis=0)\n",
    "y = new_df.target.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que las dimensiones de la matriz son 176x396900 estas podrían generar problemas.El primer problema es que al tener tantos dimensiones costará mas la visualización de los datos para realizar un entendimiento de ellos, el segundo es el tiempo que se demorará una maquina en procesar todos los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f)\n",
    "Se realiza la transformada de fourier discreta para pasar los datos desde el dominio de tiempos al dominio de frecuencias en la señal de sonido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_fourier = np.abs(np.fft.fft(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g)\n",
    "Se realiza un muetreo a través de una técnica de muestreo especializada en secuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "X_resampled = []\n",
    "for i in range(X_fourier.shape[0]):\n",
    "    sequence = X_fourier[i,:].copy()\n",
    "    resampled_sequence = signal.resample(sequence, 100000)\n",
    "    X_resampled.append(resampled_sequence)\n",
    "X_resampled = np.array(X_resampled)\n",
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso se realiza un beneficio pues se disminuye la dimensionalidad de esta forma el aprendizaje de la maquina se demorará menos ademas de que se mantendrá la información mas relevante.\n",
    "\n",
    "Se puede demostrar que el muestreo es representativo a continuación:\n",
    "Primero se muestra un histograma con todos las frecuencias para cada tipo sin la disminución de dimensionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "c=0\n",
    "for i in y:\n",
    "    if i==0:\n",
    "        d1.extend(X_fourier[c])\n",
    "    if i==1:\n",
    "        d2.extend(X_fourier[c])\n",
    "    if i==2:\n",
    "        d3.extend(X_fourier[c])\n",
    "plt.hist([d1, d2, d3],color=[\"r\",\"y\",\"b\"],label=[\"artifact\",\"murmur\",\"normal/extrahls\"],alpha=0.7)\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se muestra un histograma con los datos ya disminuidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "c=0\n",
    "for i in y:\n",
    "    if i==0:\n",
    "        d1.extend(X_resampled[c])\n",
    "    if i==1:\n",
    "        d2.extend(X_resampled[c])\n",
    "    if i==2:\n",
    "        d3.extend(X_resampled[c])\n",
    "plt.hist([d1, d2, d3],color=[\"r\",\"y\",\"b\"],label=[\"artifact\",\"murmur\",\"normal/extrahls\"],alpha=0.7)\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar que se mantiene la proporción de los datos por lo que la muestra sigue siendo representativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h)\n",
    "Se genera un conjunto de pruebas mediante hold-out validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i)\n",
    "Se estandarizan los datos para ser trabajados, lo cual es necesario para hacer reducción de dimensionalidad pues PCA asume que los datos se encuentran centrados en 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train)\n",
    "X_test = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## j)\n",
    "Se realiza una reducción de dimensionalidad utilizando PCA, representando los datos en 2 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "d=2\n",
    "pca_model = PCA(n_components=d)\n",
    "pca_model.fit(X_train)\n",
    "X_pca_train = pca_model.transform(X_train)\n",
    "X_pca_test = pca_model.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se visualizan en 2 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=0\n",
    "h1=0\n",
    "h2=0\n",
    "h3=0\n",
    "for i in X_pca_train:\n",
    "    if y_train[c]==0:\n",
    "        if h1==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"D\", c=\"y\", alpha=0.7,label=\"artifact\")\n",
    "            h1=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"D\", c=\"y\", alpha=0.7)\n",
    "    if y_train[c]==1:\n",
    "        if h2==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"^\", c=\"b\", alpha=0.7,label=\"normal/extrahls\")\n",
    "            h2=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"^\", c=\"b\", alpha=0.7)\n",
    "    if y_train[c]==2:\n",
    "        if h3==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"+\", c=\"r\",alpha=0.7,label=\"murmur\")\n",
    "            h3=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"+\", c=\"r\",alpha=0.7)\n",
    "    c+=1\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que la mayoría de los datos se encuentras agrupados cerca del 0,siendo los datos mas dispersos correspondientes a los del tipo artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k)\n",
    "Se entrena un modelo de regresión logistica variando el parametro C como 0.0001,0.01,0.1,1,10,100 y 1000. Mostrando un gráfico resumen del error en función del hiper-parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cs = [0.0001,0.01,0.1,1,10,100,1000]\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "def train_model1(param):\n",
    "    model=LR() #define your model\n",
    "    model.set_params(C=param,penalty='l2')\n",
    "    model.fit(X_pca_train,y_train)\n",
    "    return model\n",
    "def grafica1():\n",
    "    e=[]\n",
    "    for n in Cs:\n",
    "        model=train_model1(n)\n",
    "        y_pred = model.predict(X_pca_test)\n",
    "        error = 1-accuracy_score(y_test,y_pred)\n",
    "        e.append(error)\n",
    "    plt.plot(Cs,e, 'g', zorder=1, lw=2)\n",
    "    plt.scatter(Cs,e,marker=\"D\", c=\"g\", alpha=0.7,label=\"Regresion logistica\")\n",
    "    plt.legend(loc='upper right')\n",
    "    print(e)\n",
    "print (grafica1())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena una máquina de soporte vectorial(SVM) con kernel lineal variando el hiper-parámetro C al igual que en la parte anterior y construyendo un gráfico resumen del error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "\n",
    "def train_model2(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='linear')\n",
    "    model.fit(X_pca_train,y_train)\n",
    "    return model\n",
    "def grafica2():\n",
    "    e=[]\n",
    "    for n in Cs:\n",
    "        model=train_model2(n)\n",
    "        y_pred = model.predict(X_pca_test)\n",
    "        error = 1-accuracy_score(y_test,y_pred)\n",
    "        e.append(error)\n",
    "    plt.plot(Cs,e, 'y', zorder=1, lw=2)\n",
    "    plt.scatter(Cs,e,marker=\"D\", c=\"y\", alpha=0.7,label=\"SVM\")\n",
    "    plt.legend(loc='upper right')\n",
    "    return e\n",
    "print (grafica2())\n",
    "plt.title(\"Regresión logistica V/S SVM\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Miss Classification Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que SVM tiene un error miss clasification mucho menor para el parametro regularizador C=1000 con el valor 0.431. Mientras que el menor error para la regresión logística se obtiene en 0.01 con el valor 0.431."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l)\n",
    "\n",
    "Gini se basa en las varianzas de cada distribución,mientras que en entropia cada hoja maneja la distribución de probabilidad, en este caso se entrenará un árbol de decision con el criterio de entropía. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Depths = range(1,80)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "def train_model3(param):\n",
    "    model= Tree() #edit the train_model function\n",
    "    model.set_params(max_depth=param,criterion='gini',splitter='best')\n",
    "    model.fit(X_pca_train,y_train)\n",
    "    return model\n",
    "def grafica3():\n",
    "    e=[]\n",
    "    for n in Depths:\n",
    "        model=train_model3(n)\n",
    "        y_pred = model.predict(X_pca_test)\n",
    "        error = 1-accuracy_score(y_test,y_pred)\n",
    "        e.append(error)\n",
    "    plt.plot(Depths,e, 'r', zorder=1, lw=2)\n",
    "    plt.scatter(Depths,e,marker=\"D\", c=\"g\", alpha=0.7,label=\"Arbol\")\n",
    "    plt.legend(loc='upper right')\n",
    "print (grafica3())\n",
    "plt.title(\"Árbol de decisión\")\n",
    "plt.xlabel(\"Profundidad\")\n",
    "plt.ylabel(\"Miss Classification Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que al cambiar la profundidad del árbol el miss classification loss es menor para profundidades entre 2 y 10, mientras que al aumentar dichas profundidades las diferencias del error varian aumentando y disminuyendo entre 0.48."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m)\n",
    "\n",
    "Se construye un grafico con los valores obtenidos de miss classification loss al cambiar las dimensiones para la proyección PCA, los cuales no pueden superar la cantidad de filas que corresponde a 176 para logistic regression con un valor de C=1000 que fue uno de los mejores valores en el proceso anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "a=range(2,20)\n",
    "err=[]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "for d in a:\n",
    "    pca_model = PCA(n_components=d)\n",
    "    pca_model.fit(X_train)\n",
    "    X_pca_train = pca_model.transform(X_train)\n",
    "    X_pca_test = pca_model.transform(X_test)\n",
    "    model=train_model1(1000)\n",
    "    y_pred = model.predict(X_pca_test)\n",
    "    error = 1-accuracy_score(y_test,y_pred)\n",
    "    print(error,)\n",
    "    err.append(error)\n",
    "plt.plot(a,err, 'g', zorder=1, lw=2)\n",
    "plt.scatter(a,err,marker=\"D\", c=\"g\", alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al aumentar el numero de dimensiones el error tiende a disminuir.\n",
    "\n",
    "Se realiza lo mismo para SVM con valor C=1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=range(2,20)\n",
    "err=[]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "for d in a:\n",
    "    pca_model = PCA(n_components=d)\n",
    "    pca_model.fit(X_train)\n",
    "    X_pca_train = pca_model.transform(X_train)\n",
    "    X_pca_test = pca_model.transform(X_test)\n",
    "    model=train_model2(1000)\n",
    "    y_pred = model.predict(X_pca_test)\n",
    "    error = 1-accuracy_score(y_test,y_pred)\n",
    "    print(error,)\n",
    "    err.append(error)\n",
    "plt.plot(a,err, 'g', zorder=1, lw=2)\n",
    "plt.scatter(a,err,marker=\"D\", c=\"g\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al variar el numero de dimensiones, para el valor de d=6 se registra el menor miss classification loss.\n",
    "\n",
    "\n",
    "Se visualiza para un arbol con profundidad 6, pues fue la mejor obtenida en l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=range(2,20)\n",
    "err=[]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "for d in a:\n",
    "    pca_model = PCA(n_components=d)\n",
    "    pca_model.fit(X_train)\n",
    "    X_pca_train = pca_model.transform(X_train)\n",
    "    X_pca_test = pca_model.transform(X_test)\n",
    "    model=train_model3(6)\n",
    "    y_pred = model.predict(X_pca_test)\n",
    "    error = 1-accuracy_score(y_test,y_pred)\n",
    "    print(error,)\n",
    "    err.append(error)\n",
    "plt.plot(a,err, 'g', zorder=1, lw=2)\n",
    "plt.scatter(a,err,marker=\"D\", c=\"g\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al variar el numero de dimensiones el árbol varia disminuyendo y aumentando sin seguir un patrón especifico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n)\n",
    " Realice otra reducci´on de dimensionalidad ahora a trav´es de la t´ecnica LDA, para representar los datos\n",
    "en d = 2 dimensiones. Recuerde que s´olo se debe ajustar con el conjunto de entrenamiento, si se muestra\n",
    "un warning explique el porqu´e. Visualice apropiadamente la proyecci´on en 2 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "model_lda = LDA(n_components=2)\n",
    "model_lda.fit(X_train,y_train)\n",
    "X_lda_train = model_lda.transform(X_train)\n",
    "X_lda_test = model_lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=0\n",
    "h1=0\n",
    "h2=0\n",
    "h3=0\n",
    "for i in X_lda_train:\n",
    "    if y_train[c]==0:\n",
    "        if h1==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"D\", c=\"y\", alpha=0.7,label=\"artifact\")\n",
    "            h1=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"D\", c=\"y\", alpha=0.7)\n",
    "    if y_train[c]==1:\n",
    "        if h2==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"^\", c=\"b\", alpha=0.7,label=\"normal/extrahls\")\n",
    "            h2=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"^\", c=\"b\", alpha=0.7)\n",
    "    if y_train[c]==2:\n",
    "        if h3==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"+\", c=\"r\",alpha=0.7,label=\"murmur\")\n",
    "            h3=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"+\", c=\"r\",alpha=0.7)\n",
    "    c+=1\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "plt.show()\n",
    "print (\"Condición\",np.linalg.cond(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El warning mostrado se debe a que hay 2 o mas variables colineares, pues la matriz está mal condicionada, pues se realiza una inversión de la matriz que al estar mal condicionada genera errores.\n",
    "A pesar de esto al graficar se ve como una gran cantidad de datos de cada tipo se encuentran concentrados y separados entre los de otros tipos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o)\n",
    "Con el prop´osito de encontrar el mejor modelo vuelva a realizar el item h) con el i) en el nuevo espacio\n",
    "generado por la representaci´on seg´un las d dimensiones de la proyecci´on LDA. Esta nueva representaci´on\n",
    "¿mejora o empeora el desempe˜no? Explique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y, test_size=0.25, random_state=42)\n",
    "\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train)\n",
    "X_test = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p)\n",
    "Se crean nuevas características para el dataset que corresponde a la cantidad de amplitudes en ciertos rangos, ademas de la amplitud maxima y mínima, debido a lo que se pudo observar en los histogramas mostrados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sonidos(a):\n",
    "    uno=0\n",
    "    dos=0\n",
    "    tres=0\n",
    "    cuatro=0\n",
    "    for i in a:\n",
    "        if abs(i)>=20000:\n",
    "            uno=+1\n",
    "        elif abs(i)>=8000:\n",
    "            dos+=1\n",
    "        elif 0>i>-8000:\n",
    "            tres+=1\n",
    "        elif 0<i<8000:\n",
    "            cuatro+=1\n",
    "    return (uno,dos,tres,cuatro)\n",
    "\n",
    "print(sonidos(df_f[\"time_series\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "d=[]\n",
    "for i in df_f[\"time_series\"]:\n",
    "    m=sonidos(i)\n",
    "    a.append(m[0])\n",
    "    b.append(m[1])\n",
    "    c.append(m[2])\n",
    "    d.append(m[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "c=0\n",
    "for i in df_f[\"time_series\"]:\n",
    "        if y[c]==0:\n",
    "            d1.extend(i)\n",
    "        if y[c]==1:\n",
    "            d2.extend(i)\n",
    "        if y[c]==2:\n",
    "            d3.extend(i)\n",
    "        c+=1\n",
    "plt.hist([d1, d2, d3],color=[\"r\",\"y\",\"b\"],label=[\"artifact\",\"murmur\",\"normal/extrahls\"],alpha=0.7)\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_f['minimo'] = new_df['time_series'].apply(min)\n",
    "df_f['maximo'] = new_df['time_series'].apply(max)\n",
    "df_f['extremos'] = a\n",
    "df_f['medios'] = b\n",
    "df_f['leves_bajos'] = c\n",
    "df_f['leves_altos'] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_f = df_f.sample(frac=1,random_state=44)\n",
    "df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_f['time_series']\n",
    "del df_f[\"file_name\"]\n",
    "del df_f[\"len_series\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis de emociones en tweets\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('text_emotion.csv')\n",
    "df.drop(['tweet_id','author'],axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se describe el dataset que se utiliza para este actividad. Corresponde a $40.000$ tweets, los que contienen: id, sentimiento asociado, autor y contenido. Sin embargo, en este paso se limpia el dataset del tweet_id y author ya que no son necesarios para el estudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sentiment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se menciona anteriormente, este dataset consta de $40000$ tweets. Los cuales tienen en total 13 distintas clases o sentimientos asociados. El sentimiento más común es **neutral** con una frecuencia de $8638$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta tabla se puede ver las 13 clases del dataset con sus respectivas frecuencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye un conjunto de entrenamiento y otro de pruebas. Esto se realiza a través de una máscara aleatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>tiffanylu know listenin bad habit earlier sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed headach ughhhh waitin call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funer ceremoni gloomi friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>want hang friend soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dannycastillo want trade someon houston ticke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty   tiffanylu know listenin bad habit earlier sta...\n",
       "1     sadness             layin n bed headach ughhhh waitin call\n",
       "2     sadness                       funer ceremoni gloomi friday\n",
       "3  enthusiasm                              want hang friend soon\n",
       "4     neutral   dannycastillo want trade someon houston ticke..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aux = df.copy()\n",
    "\n",
    "import nltk\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "punc = (u'!', u'-', u'_', u'(', u')', u',', u'.', u':', u';', u'\"', u'\\'', u'?', u'#', u'@', u'$', u'^', u'&', u'*', u'+', u'=', u'{', u'}', u'[', u']', u'\\\\', u'|', u'<', u'>', u'/', u'—',u'...')\n",
    "\n",
    "words = stop.union(punc)\n",
    "\n",
    "count = df_aux.count().content\n",
    "\n",
    "# This for takes a long time. It apply stop word removal, lower casing, deleted puntuaction and stemming. \n",
    "\n",
    "for i in range(count):\n",
    "    tweet = \"\"\n",
    "    for j in word_tokenize(df_aux.content[i].decode('utf-8').lower()):\n",
    "        if j not in words:\n",
    "            tweet += \" \" + porter_stemmer.stem(j)\n",
    "\n",
    "    df_aux.content[i] = tweet\n",
    "    \n",
    "df_aux.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el procesamiento de los tweets se aplica lo siguiente gracias a la librería nltk.\n",
    "<ol>\n",
    "<li>Minúscula a todo el texto</li>\n",
    "<li>Se elimina toda puntuación del texto</li>\n",
    "<li>Se eliminan stop words (articulos, pronombres, preposiciones, etc)</li>\n",
    "<li>Stemming, es decir la reducción de todas las palabras a su tronco léxico base</li>\n",
    "</ol>\n",
    "\n",
    "En el último cuadro se puede ver un ejemplo.\n",
    "\n",
    "**Nota:** Para poder utilizar la librería nltk es necesario tener descargadas las \"stopwords\" que se utilizan en el último código. Estas se pueden obtener descomentando la siguiente línea de código: `nltk.download(\"stopwords\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative = ['worry', 'sadness', 'hate', 'empty', 'boredom', 'anger']\n",
    "positive = ['happiness', 'love', 'surprise', 'fun', 'relief', 'enthusiasm']\n",
    "\n",
    "df_binary = df_aux.copy()\n",
    "\n",
    "for i in range(count):\n",
    "    if df_binary.sentiment[i] in positive:\n",
    "        df_binary.sentiment[i] = 1\n",
    "    elif df_binary.sentiment[i] in negative:\n",
    "        df_binary.sentiment[i] = -1\n",
    "        \n",
    "df_binary = df_binary[df_binary.sentiment != 'neutral']\n",
    "df_binary.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer la reducción binaria al problema se considera: **worry, sadness, hate, empty, boredom y anger** como sentimientos negativos. Mientras que los sentimientos **happiness, love, surprise, fun, relief y enthusiasm** quedan como sentiemientos positivos. Así queda una distribución pareja entre ambos, con solo $764$ tweets de diferencia. Por otro lado, los tweets con sentimiento **neutral**, no serán considerados dada su ambiguedad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define nuevamente la mascara con el nuevo conjunto de datos de clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "msk = np.random.rand(len(df_binary)) < 0.8\n",
    "df_train = df_binary[msk]\n",
    "df_test = df_binary[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta sección al tratarse de la clasificación de un trozo de texto, es necesario hacer un trabajo antes para que así se pueda entrenar el conjunto de datos, donde se representa los tweets como vectores de características *features*. Se requiere contar cuantas veces aparacen ciertas palabras, para esto se construye un vocabulario, el cual es construido por la unión de todas las palabras que aparecen en los tweets. Se utiliza las librerías de *sklearn*, *feature extraction in text*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye primero un vector, el cual tiene dos parámetros `min_df` y `max_df`. Estos se refieren a la cantidad mínima y máxima de veces que debe aparacer una palabra en los tweets para que sea considerado en los tweets. En este caso se decide que debe aparacer por lo menos en un $0.1\\%$ y máximo $10\\%$. Esto hace que se eliminen palabras que probablemente no nos interesan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starting with the CountVectorizer/TfidfTransformer approach...\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "cvec = CountVectorizer(min_df=0.0001, max_df=0.1)\n",
    "cvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, se genrea un vocabulario. En el siguiente código se muestra una lista de algunos de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate all the n-grams found in all documents\n",
    "from itertools import islice\n",
    "cvec_train = cvec.fit_transform(df_train.content)\n",
    "list(islice(cvec.vocabulary_.items(), 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El largo de nuestro vocabulario resultante es de $5779$ palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(cvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza el mismo procedimiento para el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvec_test = cvec.transform(df_test.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podría ser de interés, para entender mejor, verificar cuales son las palabras que más ocurrencias tienen. Esto se puede ver en la siguiente tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "occ = np.asarray(cvec_test.sum(axis=0)).ravel().tolist()\n",
    "counts_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences': occ})\n",
    "counts_df.sort_values(by='occurrences', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Tf-idf se puede saber la frecuencia de cada término. Gracias a esto se obtiene el peso que tiene cada palabra en el texto, además serán los arreglos utilizados en los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformed_train = transformer.fit_transform(cvec_train)\n",
    "transformed_test = transformer.transform(cvec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, como información extra, en el siguiente cuadro se puede ver las palabras que tienen un mayor peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.asarray(transformed_train.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los clasificadores que se utilizarán serán:\n",
    "     \n",
    "   1. SVM  Lineal\n",
    "   2. Árbol de decisión con criterio Gini\n",
    "   3. SVM con kernel rbf\n",
    "   4. Multinomial Naive Bayes \n",
    "   5. Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = transformed_train\n",
    "y_train = np.asarray(df_train.sentiment.values).ravel().tolist()\n",
    "x_test = transformed_test\n",
    "y_test = np.asarray(df_test.sentiment.values).ravel().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model1 = LinearSVC()\n",
    "model1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "\n",
    "model2= Tree()\n",
    "model2.set_params(criterion='gini',splitter='best')\n",
    "model2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model3 = GaussianNB()\n",
    "model3.fit(x_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model4 = MultinomialNB()\n",
    "model4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "model5=LR()\n",
    "model5.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1_tr, result1_t  = model1.predict(x_train), model1.predict(x_test)\n",
    "result2_tr, result2_t  = model2.predict(x_train), model2.predict(x_test)\n",
    "result3_tr, result3_t  = model3.predict(x_train.toarray()), model3.predict(x_test.toarray())\n",
    "result4_tr, result4_t  = model4.predict(x_train), model4.predict(x_test)\n",
    "result5_tr, result5_t  = model5.predict(x_train), model5.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "miss1_tr = (1-accuracy_score(y_train, result1_tr))\n",
    "miss1_t = (1-accuracy_score(y_test, result1_t))\n",
    "\n",
    "miss2_tr = (1-accuracy_score(y_train, result2_tr))\n",
    "miss2_t = (1-accuracy_score(y_test, result2_t))\n",
    "\n",
    "miss3_tr = (1-accuracy_score(y_train, result3_tr))\n",
    "miss3_t = (1-accuracy_score(y_test, result3_t))\n",
    "\n",
    "miss4_tr = (1-accuracy_score(y_train, result4_tr))\n",
    "miss4_t = (1-accuracy_score(y_test, result4_t))\n",
    "\n",
    "miss5_tr = (1-accuracy_score(y_train, result5_tr))\n",
    "miss5_t = (1-accuracy_score(y_test, result5_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n_groups = 5\n",
    "\n",
    "miss_tr = (miss1_tr, miss2_tr, miss3_tr, miss4_tr, miss5_tr)\n",
    "\n",
    "miss_t = (miss1_t, miss2_t, miss3_t, miss4_t, miss5_t)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, miss_tr, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 error_kw=error_config,\n",
    "                 label='Training')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, miss_t, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='r',\n",
    "                 error_kw=error_config,\n",
    "                 label='Test')\n",
    "\n",
    "plt.xlabel('Clasificadores')\n",
    "plt.ylabel('Missclasification')\n",
    "plt.xticks(index + bar_width / 2, ('Linear SVM', 'Tree', 'NB', 'Multinomial NB', 'LR'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gráfico anterior se puede ver el comportamiento, con el porcentaje de datos mal clasificados de cada clasificador, tanto para el conjunto de entremaiento como de pruebas.\n",
    "\n",
    "Para este caso la Regresión Logística es la que entrega mejores resultados para el conjunto de pruebas, el cual es el que más interesa al ser datos nuevos. Se debe notar también un efecto que ocurre con el clasificador de Árbol de decisión. Este es muy bueno para el conjunto de entrenamiento pero uno de los peores para el conjunto de pruebas. Esto corrobora la teoría que dice que este clasificador sufre de un alto overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el classification report ofrecido por las métricas de sklearn, se debe definir los siguientes conceptos:\n",
    "\n",
    "**Precision:** Este se refiere a que tan acertado estuvo la clasificación de cierta clase. Por ejemplo si predijo que existían 10 tweets positivos, pero al final solo 7 de esos lo eran y los otros tres corresponden a falsos positivos, entonces la precisión será $7/10$.\n",
    "\n",
    "**Recall:** A diferencia de precision, saca un porcentaje con respecto a todo el conjunto de datos. Supongamos nuevamente los 7 tweets predecidos correctamente positivos, pero en todo el conjunto de datos existen 15 tweets positivos. Por lo que el recall será de $7/15$\n",
    "\n",
    "**F1-score:** Es una medida que combina la precisión con el recall. Es el promedio harmonico entre estos dos. Se calcula de la siguiente manera:\n",
    "\n",
    "$$F = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}$$\n",
    "\n",
    "**Support:** Indica la cantidad de elementos por conjunto en los datos reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def score_the_model(model,x,y,xt,yt):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print \"Training Accuracy: %f\"%(acc_tr)\n",
    "    print \"Test Accuracy: %f\"%(acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_the_model(model1, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_the_model(model2, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_the_model(model3, x_train.toarray(), y_train, x_test.toarray(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_the_model(model4, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_the_model(model5, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como se ve en el gráfico de la pregunta anterior cualitativamente, la Regresión Logística tiene el mejor comportamiento. Gracias a las métricas de sklearn esto se puede comprobar cuantitativamente, además se puede notar que es levemente superior a SVM lineal. \n",
    "\n",
    "A pesar de que el Árbol de decisión empeora bastante comparado con el conjunto de entrenamiento, este clasificador sigue siendo mejor que Naive Bayes, el cual para este caso es el que se obtiene peores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se replica el procesamiento anteriormente realizado en el punto c) y en el punto e). Con la diferencia que ahora se tendrán múltiples clases. Se enumeran estas desde el 0 al 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     8638\n",
       "7     8459\n",
       "1     5209\n",
       "8     5165\n",
       "2     3842\n",
       "3     2187\n",
       "4     1776\n",
       "5     1526\n",
       "9     1323\n",
       "10     827\n",
       "6      759\n",
       "11     179\n",
       "12     110\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = ['happiness', 'love', 'surprise', 'fun', 'relief', 'enthusiasm', 'worry', 'sadness', 'hate', 'empty', 'boredom', 'anger']\n",
    "\n",
    "df_mul = df_aux.copy()\n",
    "\n",
    "for i in range(count):\n",
    "    if df_mul.sentiment[i] == 'neutral':\n",
    "        df_mul.sentiment[i] = 0\n",
    "    else:\n",
    "        df_mul.sentiment[i] = 1 + emotions.index(df_mul.sentiment[i])\n",
    "        \n",
    "df_mul.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "msk = np.random.rand(len(df_mul)) < 0.8\n",
    "df_train = df_mul[msk]\n",
    "df_test = df_mul[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(min_df=100, max_df=0.1)\n",
    "X_train_counts = count_vect.fit_transform(df_train.content)\n",
    "X_test_counts = count_vect.transform(df_test.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = X_train_tfidf\n",
    "y_train = np.asarray(df_train.sentiment.values).ravel().tolist()\n",
    "x_test = X_test_tfidf\n",
    "y_test = np.asarray(df_test.sentiment.values).ravel().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se utilizan los clasificadores que por defecto son extendidos desde clasificación binaria a múltiple. En este caso serán **KNN** y **Multinomial Naive Bayes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    " \n",
    "model11 = KNeighborsClassifier()\n",
    "model11.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model12 = MultinomialNB()\n",
    "model12.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result11_tr, result11_t  = model11.predict(x_train), model11.predict(x_test)\n",
    "result12_tr, result12_t  = model12.predict(x_train), model12.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "miss11_tr = (1-accuracy_score(y_train, result11_tr))\n",
    "miss11_t = (1-accuracy_score(y_test, result11_t))\n",
    "\n",
    "miss12_tr = (1-accuracy_score(y_train, result12_tr))\n",
    "miss12_t = (1-accuracy_score(y_test, result12_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0c8da990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n_groups = 2\n",
    "\n",
    "miss_tr = (miss11_tr, miss12_tr)\n",
    "\n",
    "miss_t = (miss11_t, miss12_t)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, miss_tr, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 error_kw=error_config,\n",
    "                 label='Training')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, miss_t, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='r',\n",
    "                 error_kw=error_config,\n",
    "                 label='Test')\n",
    "\n",
    "plt.xlabel('Clasificadores')\n",
    "plt.ylabel('Missclasification')\n",
    "plt.xticks(index + bar_width / 2, ('kNN', 'Multinomial NB'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver el error de missclasification de ambos es altísimo, por lo que no son considerados buenas medidas de clasificación, a pesar de Multinomial NB es levemente mejor. La razón de estos pueden ser debidos a mala configuración por parte del experimentador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se utilizan clasificadores que por lo general son exclusivamente binarios. Estos se extienden a través de la técnica **One vs Rest (OvR)** y **One vs One (OvO)**. \n",
    "\n",
    "La estrategia de **One vs Rest** consiste en ajustar un clasificador por clase. Por cada clasificador, cada clase es ajustada en contra de todo el resto de las clases. Es computacionalmente más eficiente, dado que se clasifica solamente las $n$ clases por lo que tiene una complejidad $O(n)$. Esta es la comúnmente usada por defecto.\n",
    "\n",
    "La estrategiea **One vs One** consiste en construir un clasificador por cada par de clases. Al momento de la predicción la clase con la mayor votación es seleccionada. Selecciona la clase con la confianza de clasificación agregada más alta al sumar los niveles de confianza de clasificación por pares calculados por los clasificadores binarios subyacentes. Como requiere construir $n \\cdot \\frac{n - 1}{2}$ clasificadores (donde $n$ es la cantidad de clases) tiene una complejidad de $O(n^{2})$ la que es mucho peor que para **OvR**.\n",
    "\n",
    "Se utilizara específicamente SVM con kernel rbf, la diferencia es que se entrenará uno con el método **OvR** y el otro con **OvO**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import SVC as SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "classif21 = OneVsRestClassifier(SVM(kernel='rbf'))\n",
    "classif21.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif22 = OneVsOneClassifier(SVM(kernel='rbf'))\n",
    "classif22.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result21_tr, result21_t  = classif21.predict(x_train), classif21.predict(x_test)\n",
    "result22_tr, result22_t  = classif22.predict(x_train), classif22.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "miss21_tr = (1-accuracy_score(y_train, result21_tr))\n",
    "miss21_t = (1-accuracy_score(y_test, result21_t))\n",
    "\n",
    "miss22_tr = (1-accuracy_score(y_train, result22_tr))\n",
    "miss22_t = (1-accuracy_score(y_test, result22_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprueba lo visto en la teoría, donde **One vs One** toma tiempo considerablemnte mayor en comparación a **One vs Rest**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gracias a la librería sklearn, la regresión logistica puede ser facilmente extendida a multiples clases a través **One vs Rest** y otro definiedo que la variable a predecir se distribuye **Multinomial**.\n",
    "\n",
    "Si es escogida la opción **One vs Rest**, entonces se ajusta un clasificador para cada clase. En cambio con **Multinomial**, la función de perdidad es la pérdida multinomial en toda la distribución de probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "model31 = LR()\n",
    "model31.set_params(multi_class = 'ovr')\n",
    "model31.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "model32 = LR()\n",
    "model32.set_params(multi_class = 'multinomial', solver = 'newton-cg')\n",
    "model32.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result31_tr, result31_t  = model31.predict(x_train), model31.predict(x_test)\n",
    "result32_tr, result32_t  = model32.predict(x_train), model32.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "miss31_tr = (1-accuracy_score(y_train, result31_tr))\n",
    "miss31_t = (1-accuracy_score(y_test, result31_t))\n",
    "\n",
    "miss32_tr = (1-accuracy_score(y_train, result32_tr))\n",
    "miss32_t = (1-accuracy_score(y_test, result32_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJCcoMzMrJCcoMzMrJCcoMzMrpP8Poo1pRzTkbkcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0d820810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n_groups = 6\n",
    "\n",
    "miss_tr = (miss11_tr, miss12_tr, miss21_tr, miss22_tr, miss31_tr, miss32_tr)\n",
    "\n",
    "miss_t = (miss11_t, miss12_t, miss21_t, miss22_t, miss31_t, miss32_t)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, miss_tr, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 error_kw=error_config,\n",
    "                 label='Training')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, miss_t, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='r',\n",
    "                 error_kw=error_config,\n",
    "                 label='Test')\n",
    "\n",
    "plt.xlabel('Clasificadores')\n",
    "plt.ylabel('Missclasification')\n",
    "plt.xticks(index + bar_width / 2, ('kNN', 'Multi NB', 'SVM ovr', 'SVM ovo','LR ovr', 'Multi LR'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en el gráfico, todos los clasificadores tienen un mal comportamiento y no son suficientes como para poder clasificar los datos de una manera que se pueda predecir. Esto probablemente se deba a una mal preprocesamiento, lo que genera que estén mal pesados luego cuando se realiza la obtención de los features y conlleve a una mala clasificación cuando son múltiples clases.\n",
    "\n",
    "Por otro lado se sigue viendo, al igual que cuando se hace la clasificación binaria, que la **Regresión Logística** es la que entrega los mejores resultados para el conjunto de prueba, seguido por **Naive Bayes Multinomial**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
