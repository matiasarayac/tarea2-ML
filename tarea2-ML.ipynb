{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2 - Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claudia Hazard 201404523-9\n",
    "## Matías Araya 201173082-8\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tipos de fronteras en Clasificación\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Se comienza creando dataset con 2 dimensiones, conformado por dos conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples=500\n",
    "mean = (0,-4)\n",
    "C = np.array([[0.3, 0.1], [0.1, 1.5]])\n",
    "datos1 = np.random.multivariate_normal(mean, C, n_samples)\n",
    "outer_circ_x = np.cos(np.linspace(0, np.pi, n_samples))*3\n",
    "outer_circ_y = np.sin(np.linspace(0, np.pi, n_samples))*3\n",
    "datos2 = np.vstack((outer_circ_x,outer_circ_y)).T\n",
    "from sklearn.utils import check_random_state\n",
    "generator = check_random_state(10)\n",
    "datos2 += generator.normal(scale=0.3, size=datos2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrega ruido al conjunto de datos para así realizar un estudio mas realista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((datos1, datos2), axis=0)\n",
    "print len(X)\n",
    "n = 20 #ruido/noise\n",
    "y1 = np.zeros(datos1.shape[0]+n)\n",
    "y2 = np.ones(datos2.shape[0]-n)\n",
    "y = np.concatenate((y1,y2),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la configuración de el código anterior existen $1000$ datos en total, los cuales $520$ corresponden a un grupo (puntos azules) y $480$ a otro (puntos verdes). Se nota como el ruido de de $20$ correspondientes realmente al grupo azul, se asemejan más a la figura del grupo verde por lo que genera ruido a la muestra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `visualize_border` es de utilidad para visualizar el conjunto de datos con su respectivo clasificador, el que se utilizará en preguntas posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_border(model,x,y,title=\"\"):\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "    plt.scatter(x[:,0], x[:,1], s=50, c=y, cmap=plt.cm.winter)\n",
    "    h = .02 # step size in the mesh\n",
    "    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) LDA\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "model_lda = LDA()\n",
    "model_lda.fit(X,y)\n",
    "visualize_border(model_lda,X,y,\"LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Linear Discriminant Analysis (LDA), como se ve en la figura mostrada, traza una linea clara que logra separar la clasificación de ambos grupos, quedando así la mayoría de los puntos azules por un lado y la totalidad de los verdes en el otro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) QDA\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "model_qda = QDA()\n",
    "model_qda.fit(X,y)\n",
    "visualize_border(model_qda,X,y,\"QDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso con Quadratic Discriminant Analysis (QDA), como lo dice su nombre al ser de tipo cuadrático, logra crear una curva asemejandose de mejor manera a la figura y cualitativamente se podría decir que clasifica mejor que LDA. Esto sin embargo conlleva un mayor costo de computación además de mayor posibilidad de overfitting, con lo que se analiza en próxima sección si vale la pena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(datos1, bins='auto')\n",
    "plt.title(\"Histograma datos 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(datos2, bins='auto')\n",
    "plt.title(\"Histograma datos 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_lda = model_lda.predict(X)\n",
    "y_pred_qda = model_qda.predict(X)\n",
    "\n",
    "y_true = y\n",
    "\n",
    "print(\"Miss Classification Loss LDA: %f\"%(1-accuracy_score(y_true, y_pred_lda)))\n",
    "print(\"Miss Classification Loss QDA: %f\"%(1-accuracy_score(y_true, y_pred_qda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto se puede ver que la diferencia entre los errores es muy pequña, donde LDA tiene un error de clasificación de $0.021$ y QDA es levemente menor con $0.020$. Con esto se puede comprobar, para este pequeño caso, que no es necesario utilizar QDA ya que genera un gasto extra además de generar un mayor overfitting y no se obtiene una mejor calidad en la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "\n",
    "def visualize_border_interactive(param):\n",
    "    model = train_model(param)\n",
    "    visualize_border(model,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "def train_model(param):\n",
    "    model=LR() #define your model\n",
    "    model.set_params(C=param,penalty='l2')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 0.1\n",
    "p_max = 10\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar en el gráfico interactivo, cambiando el paramétro se mueve ligeramente la línea divisiora de la regresión que separa ambas clases. Esto se puede asemejar a lo que hace Ridge o Lasso, donde se penalizan los coeficientes restando importancia a los que influyen de menor manera en el modelo. Así, si el parámetro es bajo, cercano a cero, tiene menos aceptación a que haya puntos mal clasificados. Con el parámetro en $0.1$ ni un punto verde queda completamente en el grupo azul. Mientras que, con un parámetro más alto, por ejemplo $10$ se nota como cambia donde la pendiente de la recta se hace más pronunciada, aceptando así algunos puntos verdes en el grupo azul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM), a diferencia de la otras técnicas que se han visto, busca minimizar la distancia a los puntos más cercanos del hiperplano. No como el resto de los métodos que minimiza los errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "\n",
    "def train_model(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='linear')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 0.1\n",
    "p_max = 1\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El efecto es casi el mismo que la Regresión Logística, donde con una valor alto de el parámetro $C$ la línea divisora queda con una mayor pendiente, aceptando así los valores azules correspondientes al ruido pero incluyendo también unos pocos del conjunto verde. Mientras que con un $C$ cercano a cero permite menos puntos azules de los correspondientes al ruido pero no acepta ni un verde en el conjunto azul. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una solución simplificada para SVM no Lineal puede ser escrita como:\n",
    "\n",
    "$$\\hat{f(x)} = \\displaystyle\\sum_{i=1}^{n} \\hat{\\alpha_i}y_i K(x,x_i) + \\hat{\\beta_0}$$\n",
    "\n",
    "Donde $K(x, x_i)$ es la función Kernel y se cumple que para todo $x_i$, $0 < \\alpha_i < C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='rbf')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 0.1\n",
    "p_max = 1\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='poly')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 0.1\n",
    "p_max = 1\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rol del parámetro $C$ es claro en un espacio con más atributos, esto dado que una separación perfecta es usualmente conseguible. Un valor grande de $C$ hará que sea más ondulado, mientras que uno más pequeño reflejará curvas más suaves.\n",
    "\n",
    "El efecto anterior explicado se puede ver de buena manera con el Kernel rbf, donde cambiando el parámetro con un valor de $1$ quedan las líneas más onduladas encerrando al conjunto verde. Mientras cuando se asigna un valor cercano a cero, $0.1$, se ve reflejado que las línas encierran al grupo azul siendo más suave las curvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## h)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "def train_model(param):\n",
    "    model= Tree() #edit the train_model function\n",
    "    model.set_params(max_depth=param,criterion='gini',splitter='best')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 1\n",
    "p_max = 4\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro que se varía es el de maxima profundidad del árbol. Primero se realiza el ánalisis con el criterio Gini, el cual es la suma de las varianzas de cada distribución. Con el parámetro igual a $1$, solo se tendrá un árbol de profundidad 1, por lo que, como se ve en la imagen, separa el conjunto de datos en $2$. Separando claramente ambos conjuntos de datos (azules y verdes).\n",
    "\n",
    "Si esta parámetro aumenta, se agrega un nivel de decisión y de profundidad, por lo que se vuelve más preciso el árbol. Como se puede ver en el código anterior, con 3 niveles es capaz de capturar gran parte del ruido en el conjunto verde a su correspondiente conjunto.\n",
    "\n",
    "La gran ventaja de este método es su simpleza para ser interpretado, pero sufre de un gran overfitting comparado con los otros métodos anteriormente vistos. Esto se puede comprobar que si el parámetro es $4$, se agrega una tercera clase inexistente con puntos azules y verdes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "def train_model(param):\n",
    "    model= Tree() #edit the train_model function\n",
    "    model.set_params(max_depth=param,criterion='entropy',splitter='best')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 2\n",
    "p_max = 8\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cambia el criterio a information gain, este intenta minimizar la suma pesada de las entropías resultantes. Este es equivalente a maximizar la ganancia de información al dividir el nodo. En otras palabras se debe buscar una medida de la pureza de cada nodo resultante.\n",
    "\n",
    "Con poca entropía es claro saber a que clase corresponde, si es muy entrópica no se tiene esa claridad.\n",
    "\n",
    "Con este criterio es notorio que se obtienen mejores resultado cuando el parámetro es alto en comparación a Gini, donde por ejemplo con $8$ logra diferenciar los puntos azules del ruido sin agregar puntos verdes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def train_model(param):\n",
    "    model = KNeighborsClassifier()\n",
    "    model.set_params(n_neighbors=param)\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 1\n",
    "p_max = 10\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análisis de audios como datos brutos\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "def clean_filename(fname, string):\n",
    "    file_name = fname.split('/')[1]\n",
    "    if file_name[:2] == '__':\n",
    "        file_name = string + file_name\n",
    "    return file_name\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "def load_wav_file(name, path):\n",
    "    s, b = wavfile.read(path + name)\n",
    "    assert s == SAMPLE_RATE\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"asd/asda/asddsa\"\n",
    "a.split(\"/\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)\n",
    "Construya un dataframe con los datos a analizar. Describa el dataset y determine cuántos registros hay\n",
    "por clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./latidos/set_a.csv')\n",
    "df.info()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset esta compuesto por 4 columnas de las cuales dataset y sublabel no aportan mucho pues dataset siempre toma el valor de a y sublabel no tiene ningun dato. En cuanto a las columnas fname y label, fname corresponde a el nombre del archivo que contiene la data, el tipo de sonido y luego el nombre del archivo wav, mientras que label corresponde al tipo de sonido nuevamente.\n",
    "Ademas hay blabla bla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "file = csv.reader(open(\"set_a.csv\"))\n",
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "d4=[]\n",
    "for _,fname,label,_ in file:\n",
    "    if \"/\" in fname:\n",
    "        a =clean_filename(fname,\"\")\n",
    "        tipo=a.split(\"__\")[0]\n",
    "        nom=a.split(\"__\")[1]\n",
    "        dat=load_wav_file(nom,\"./latidos/\")\n",
    "        if tipo==\"normal\":\n",
    "            d1.extend(dat)\n",
    "        if tipo==\"murmur\":\n",
    "            d2.extend(dat)\n",
    "        if tipo==\"artifact\":\n",
    "            d3.extend(dat)\n",
    "        if tipo==\"extrahls\":\n",
    "            d4.extend(dat)        \n",
    "plt.hist([d1, d2, d3, d4],color=[\"r\",\"y\",\"b\",\"g\"],label=[\"Normal\",\"Murmur\",\"Artifact\",\"Extra Heart Sound\"],alpha=0.7)\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se analiza como son los archivos wav:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c1,c2,c3,c4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "Lea los archivos .wav y transformelos en secuencias de tiempo. Realice un padding de ceros al final de\n",
    "cada secuencia para que todas queden representadas con la misma cantidad de elementos, explique la\n",
    "importancia de realizar este paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padd_zeros(array,length):\n",
    "    aux = np.zeros(length)\n",
    "    aux[:array.shape[0]] = array\n",
    "    return aux\n",
    "new_df =pd.DataFrame({'file_name' : df['fname'].apply(clean_filename,string='Aunlabelledtest')})\n",
    "new_df['time_series'] = new_df['file_name'].apply(load_wav_file, path='./latidos/')\n",
    "new_df['len_series'] = new_df['time_series'].apply(len)\n",
    "new_df['time_series']=new_df['time_series'].apply(padd_zeros,length=max(new_df['len_series']))\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len_series corresponde a la cantidad de datos tomados, es decir, tiempo para cada archivo wav. Es necesario realizar un padding para tener la misma cantidad de elementos a comparar, pues no es lo mismo la proporcion de tener 1 dato de tipo a y 3 de tipo b comparandolo con  400 datos de tipo a y 50 de tipo b. duracion de los audios secuencias de tiempos. para que tengan el mismo largo. misma cantidad de atributos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n",
    "Manipule los datos y cambie las etiquetas de los audios por otras asignadas por un doctor experto [4],\n",
    "el cual afirma que estos cambios son requeridos. Vuelva a determinar cu´antos registros hay por clase.\n",
    "N´otese que ahora son 3 clases ¿Explique la problem´atica de tener etiquetas mal asignadas en los datos?\n",
    "¿Un solo dato puede afectar esto?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_labels =[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2,\n",
    "2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
    "1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0,\n",
    "2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0,\n",
    "0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "labels = ['artifact','normal/extrahls', 'murmur']\n",
    "new_df['target'] = [labels[i] for i in new_labels]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depende del tipode maquina que se esta utilizando sbm optimizacion de frontera soft margin es distinto, arbol recubridor tampoco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)\n",
    "Codifique las distintas clases a valores num´ericos para que puedan ser trabajados por los algoritmos\n",
    "clasificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df[\"target\"] = new_df[\"target\"].astype('category')\n",
    "cat_columns = new_df.select_dtypes(['category']).columns\n",
    "new_df[cat_columns] = new_df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)\n",
    " Desordene los datos, evitando as´ı el orden en el que vienen la gran mayor´ıa de las etiquetas. Cree la\n",
    "matriz que conforma a los datos en sus dimensiones sin preprocesar, es decir, cada ejemplo es una\n",
    "secuencia de amplitudes en el tiempo. ¿Las dimensiones de ´esta indica que puede generar problemas?\n",
    "¿De qué tipo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = new_df.sample(frac=1,random_state=44)\n",
    "X = np.stack(new_df['time_series'].values, axis=0)\n",
    "y = new_df.target.values\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f)\n",
    "Para pre procesar la secuencia en el tiempo realice una transformada de fourier discreta [5] para pasar\n",
    "los datos desde el dominio de tiempos al dominio de frecuencias presentes en la se˜nal de sonido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_fourier = np.abs(np.fft.fft(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g)\n",
    "Para seguir con el pre procesamiento realice un muestreo representativo de los datos a trav´es de una\n",
    "t´ecnica de muestreo especializada en secuencias ¿En qu´e beneficia este paso? ¿C´omo podr´ıa determinar\n",
    "si el muestro es representativo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "X_resampled = []\n",
    "for i in range(X_fourier.shape[0]):\n",
    "sequence = X_fourier[i,:].copy()\n",
    "resampled_sequence = signal.resample(sequence, 100000)\n",
    "X_resampled.append(resampled_sequence)\n",
    "X_resampled = np.array(X_resampled)\n",
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h)\n",
    "Genere un conjunto de pruebas mediante la t´ecnica hold-out validation para v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i)\n",
    "Realice un proceso de estandarizar los datos para ser trabajados adecuadamente. Recuerde que solo se\n",
    "debe ajustar (calcular media y desviaci´on est´andar) con el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train)\n",
    "X_test = std.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## j)\n",
    "Realice una reducci´on de dimensionalidad a trav´es de la t´ecnica PCA, para representar los datos en\n",
    "d = 2 dimensiones. Recuerde que solo se debe ajustar (encontrar las componentes principales) con el\n",
    "conjunto de entrenamiento. Visualice apropiadamente la proyecci´on en 2 dimensiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "d=2\n",
    "pca_model = PCA(n_components=d)\n",
    "pca_model.fit(X_train)\n",
    "X_pca_train = pca_model.transform(X_train)\n",
    "X_pca_test = pca_model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=0\n",
    "h1=0\n",
    "h2=0\n",
    "h3=0\n",
    "for i in X_pca_train:\n",
    "    if y_train[c]==0:\n",
    "        if h1==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"D\", c=\"y\", alpha=0.7,label=\"artifact\")\n",
    "            h1=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"D\", c=\"y\", alpha=0.7)\n",
    "    if y_train[c]==1:\n",
    "        if h2==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"^\", c=\"b\", alpha=0.7,label=\"normal/extrahls\")\n",
    "            h2=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"^\", c=\"b\", alpha=0.7)\n",
    "    if y_train[c]==2:\n",
    "        if h3==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"+\", c=\"r\",alpha=0.7,label=\"murmur\")\n",
    "            h3=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"+\", c=\"r\",alpha=0.7)\n",
    "    c+=1\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k)\n",
    "Entrene un modelo de Regresi´on Log´ıstica variando el par´ametro de regularizacion C construyendo un\n",
    "gr´afico resumen del error en funci´on de este hiper-par´ametro. Adem´as entrene una M´aquina de Soporte\n",
    "Vectorial (SVM) con kernel lineal, variando el hiper-par´ametro de regularizacion C en el mismo rango\n",
    "que para la Regresi´on Log´ıstica, construyendo el mismo gr´afico resumen. Compare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cs = [0.0001,0.01,0.1,1,10,100,1000]\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "def train_model1(param):\n",
    "    model=LR() #define your model\n",
    "    model.set_params(C=param,penalty='l2')\n",
    "    model.fit(X_pca_train,y_train)\n",
    "    return model\n",
    "\n",
    "def grafica1():\n",
    "    e=[]\n",
    "    for n in Cs:\n",
    "        model=train_model1(n)\n",
    "        y_pred = model.predict(X_pca_train)\n",
    "        error = 1-accuracy_score(y_train,y_pred)\n",
    "        e.append(error)\n",
    "    plt.plot(Cs,e, 'r', zorder=1, lw=2)\n",
    "    plt.scatter(Cs,e,marker=\"D\", c=\"g\", alpha=0.7,label=\"Promedio de error:\"+str(sum(e)/len(e)))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "print (grafica1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "\n",
    "def train_model2(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='linear')\n",
    "    model.fit(X_pca_train,y_train)\n",
    "    return model\n",
    "def grafica2():\n",
    "    e=[]\n",
    "    for n in Cs:\n",
    "        model=train_model2(n)\n",
    "        y_pred = model.predict(X_pca_train)\n",
    "        error = 1-accuracy_score(y_train,y_pred)\n",
    "        e.append(error)\n",
    "    plt.plot(Cs,e, 'r', zorder=1, lw=2)\n",
    "    plt.scatter(Cs,e,marker=\"D\", c=\"g\", alpha=0.7,label=\"Promedio de error:\"+str(sum(e)/len(e)))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "print (grafica2())\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l)\n",
    "Entrene un Arbol de Decisi´on, con la configuraci´on que estime conveniente, variando el hiper-par´ametro ´\n",
    "regularizador max depth, construyendo un gr´afico resumen del error en funci´on de este par´ametro.\n",
    "Compare con los modelos anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Depths = range(1,30)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "def train_model3(param):\n",
    "    model= Tree() #edit the train_model function\n",
    "    model.set_params(max_depth=param,criterion='entropy',splitter='best')\n",
    "    model.fit(X_pca_train,y_train)\n",
    "    return model\n",
    "def grafica3():\n",
    "    e=[]\n",
    "    for n in Depths:\n",
    "        model=train_model3(n)\n",
    "        y_pred = model.predict(X_pca_train)\n",
    "        error = 1-accuracy_score(y_train,y_pred)\n",
    "        e.append(error)\n",
    "    plt.plot(Depths,e, 'r', zorder=1, lw=2)\n",
    "    plt.scatter(Depths,e,marker=\"D\", c=\"g\", alpha=0.7,label=\"Promedio de error:\"+str(sum(e)/len(e)))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "print (grafica3())\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m)\n",
    "Experimente con diferentes dimensiones d para la proyecci´on de PCA con el prop´osito de obtener un\n",
    "modelo con menor error. Construya una tabla o gr´afico resumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "a=range(-3,4)\n",
    "valores=[]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "for i in a:\n",
    "    valores.append(10**i)\n",
    "for d in valores:\n",
    "    model = PCA(n_components=d)\n",
    "    pca_model.fit(X_train)\n",
    "    X_pca_train = pca_model.transform(X_train)\n",
    "    X_pca_test = pca_model.transform(X_test)\n",
    "    # row and column sharing\n",
    "    grafica1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valores=[]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "for i in a:\n",
    "    valores.append(10**i)\n",
    "for d in valores:\n",
    "    model = PCA(n_components=d)\n",
    "    pca_model.fit(X_train)\n",
    "    X_pca_train = pca_model.transform(X_train)\n",
    "    X_pca_test = pca_model.transform(X_test)\n",
    "    # row and column sharing\n",
    "    grafica2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valores=[]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "for i in a:\n",
    "    valores.append(10**i)\n",
    "for d in valores:\n",
    "    model = PCA(n_components=d)\n",
    "    pca_model.fit(X_train)\n",
    "    X_pca_train = pca_model.transform(X_train)\n",
    "    X_pca_test = pca_model.transform(X_test)\n",
    "    # row and column sharing\n",
    "    grafica3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n)\n",
    " Realice otra reducci´on de dimensionalidad ahora a trav´es de la t´ecnica LDA, para representar los datos\n",
    "en d = 2 dimensiones. Recuerde que s´olo se debe ajustar con el conjunto de entrenamiento, si se muestra\n",
    "un warning explique el porqu´e. Visualice apropiadamente la proyecci´on en 2 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "model_lda = LDA(n_components=2)\n",
    "model_lda.fit(X_train,y_train)\n",
    "X_pca_train = model_lda.transform(X_train)\n",
    "X_pca_test = model_lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=0\n",
    "h1=0\n",
    "h2=0\n",
    "h3=0\n",
    "for i in X_lda_train:\n",
    "    if y_train[c]==0:\n",
    "        if h1==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"D\", c=\"y\", alpha=0.7,label=\"artifact\")\n",
    "            h1=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"D\", c=\"y\", alpha=0.7)\n",
    "    if y_train[c]==1:\n",
    "        if h2==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"^\", c=\"b\", alpha=0.7,label=\"normal/extrahls\")\n",
    "            h2=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"^\", c=\"b\", alpha=0.7)\n",
    "    if y_train[c]==2:\n",
    "        if h3==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"+\", c=\"r\",alpha=0.7,label=\"murmur\")\n",
    "            h3=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"+\", c=\"r\",alpha=0.7)\n",
    "    c+=1\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o)\n",
    "Con el prop´osito de encontrar el mejor modelo vuelva a realizar el item h) con el i) en el nuevo espacio\n",
    "generado por la representaci´on seg´un las d dimensiones de la proyecci´on LDA. Esta nueva representaci´on\n",
    "¿mejora o empeora el desempe˜no? Explique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y, test_size=0.25, random_state=42)\n",
    "\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train)\n",
    "X_test = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p)\n",
    "Intente mejorar el desempe˜no de los algoritmos ya entrenados. Dise˜ne ahora sus propias cracter´ısticas\n",
    "(feature crafting) a partir de los datos brutos (secuencia de amplitudes), puede inspirarse en otros\n",
    "trabajos [6] [7] si desea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis de emociones en tweets\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('text_emotion.csv')\n",
    "df.drop(['tweet_id','author'],axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se describe el dataset que se utiliza para este actividad. Corresponde a $40.000$ tweets, los que contienen: id, sentimiento asociado, autor y contenido. Sin embargo, en este paso se limpia el dataset del tweet_id y author ya que no son necesarios para el estudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se menciona anteriormente, este dataset consta de $40000$ tweets. Los cuales tienen en total 13 distintas clases o sentimientos asociados. El sentimiento más común es **neutral** con una frecuencia de $8638$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta tabla se puede ver las 13 clases del dataset con sus respectivas frecuencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye un conjunto de entrenamiento y otro de pruebas. Esto se realiza a través de una máscara aleatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>tiffanylu know listenin bad habit earlier sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed headach ughhhh waitin call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funer ceremoni gloomi friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>want hang friend soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dannycastillo want trade someon houston ticke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty   tiffanylu know listenin bad habit earlier sta...\n",
       "1     sadness             layin n bed headach ughhhh waitin call\n",
       "2     sadness                       funer ceremoni gloomi friday\n",
       "3  enthusiasm                              want hang friend soon\n",
       "4     neutral   dannycastillo want trade someon houston ticke..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aux = df.copy()\n",
    "\n",
    "import nltk\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "punc = (u'!', u'-', u'_', u'(', u')', u',', u'.', u':', u';', u'\"', u'\\'', u'?', u'#', u'@', u'$', u'^', u'&', u'*', u'+', u'=', u'{', u'}', u'[', u']', u'\\\\', u'|', u'<', u'>', u'/', u'—',u'...')\n",
    "\n",
    "words = stop.union(punc)\n",
    "\n",
    "count = df_aux.count().content\n",
    "\n",
    "# This for takes a long time. It apply stop word removal, lower casing, deleted puntuaction and stemming. \n",
    "\n",
    "for i in range(count):\n",
    "    tweet = \"\"\n",
    "    for j in word_tokenize(df_aux.content[i].decode('utf-8').lower()):\n",
    "        if j not in words:\n",
    "            tweet += \" \" + porter_stemmer.stem(j)\n",
    "\n",
    "    df_aux.content[i] = tweet\n",
    "    \n",
    "df_aux.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el procesamiento de los tweets se aplica lo siguiente gracias a la librería nltk.\n",
    "<ol>\n",
    "<li>Minúscula a todo el texto</li>\n",
    "<li>Se elimina toda puntuación del texto</li>\n",
    "<li>Se eliminan stop words (articulos, pronombres, preposiciones, etc)</li>\n",
    "<li>Stemming, es decir la reducción de todas las palabras a su tronco léxico base</li>\n",
    "</ol>\n",
    "\n",
    "En el último cuadro se puede ver un ejemplo.\n",
    "\n",
    "**Nota:** Para poder utilizar la librería nltk es necesario tener descargadas las \"stopwords\" que se utilizan en el último código. Estas se pueden obtener descomentando la siguiente línea de código: `nltk.download(\"stopwords\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    16063\n",
       " 1    15299\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative = ['worry', 'sadness', 'hate', 'empty', 'boredom', 'anger']\n",
    "positive = ['happiness', 'love', 'surprise', 'fun', 'relief', 'enthusiasm']\n",
    "\n",
    "df_binary = df_aux.copy()\n",
    "\n",
    "for i in range(count):\n",
    "    if df_binary.sentiment[i] in positive:\n",
    "        df_binary.sentiment[i] = 1\n",
    "    elif df_binary.sentiment[i] in negative:\n",
    "        df_binary.sentiment[i] = -1\n",
    "        \n",
    "df_binary = df_binary[df_binary.sentiment != 'neutral']\n",
    "df_binary.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer la reducción binaria al problema se considera: **worry, sadness, hate, empty, boredom y anger** como sentimientos negativos. Mientras que los sentimientos **happiness, love, surprise, fun, relief y enthusiasm** quedan como sentiemientos positivos. Así queda una distribución pareja entre ambos, con solo $764$ tweets de diferencia. Por otro lado, los tweets con sentimiento **neutral**, no serán considerados dada su ambiguedad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define nuevamente la mascara con el nuevo conjunto de datos de clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "msk = np.random.rand(len(df_binary)) < 0.8\n",
    "df_train = df_binary[msk]\n",
    "df_test = df_binary[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta sección al tratarse de la clasificación de un trozo de texto, es necesario hacer un trabajo antes para que así se pueda entrenar el conjunto de datos, donde se representa los tweets como vectores de características *features*. Se requiere contar cuantas veces aparacen ciertas palabras, para esto se construye un vocabulario, el cual es construido por la unión de todas las palabras que aparecen en los tweets. Se utiliza las librerías de *sklearn*, *feature extraction in text*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye primero un vector, el cual tiene dos parámetros `min_df` y `max_df`. Estos se refieren a la cantidad mínima y máxima de veces que debe aparacer una palabra en los tweets para que sea considerado en los tweets. En este caso se decide que debe aparacer por lo menos en un $0.1\\%$ y máximo $10\\%$. Esto hace que se eliminen palabras que probablemente no nos interesan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with the CountVectorizer/TfidfTransformer approach...\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "cvec = CountVectorizer(min_df=0.0001, max_df=0.1)\n",
    "cvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, se genrea un vocabulario. En el siguiente código se muestra una lista de algunos de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all the n-grams found in all documents\n",
    "from itertools import islice\n",
    "cvec_train = cvec.fit_transform(df_train.content)\n",
    "list(islice(cvec.vocabulary_.items(), 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El largo de nuestro vocabulario resultante es de $5779$ palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza el mismo procedimiento para el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvec_test = cvec.transform(df_test.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podría ser de interés, para entender mejor, verificar cuales son las palabras que más ocurrencias tienen. Esto se puede ver en la siguiente tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ = np.asarray(cvec_test.sum(axis=0)).ravel().tolist()\n",
    "counts_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences': occ})\n",
    "counts_df.sort_values(by='occurrences', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Tf-idf se puede saber la frecuencia de cada término. Gracias a esto se obtiene el peso que tiene cada palabra en el texto, además serán los arreglos utilizados en los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformed_train = transformer.fit_transform(cvec_train)\n",
    "transformed_test = transformer.transform(cvec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, como información extra, en el siguiente cuadro se puede ver las palabras que tienen un mayor peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.asarray(transformed_train.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los clasificadores que se utilizarán serán:\n",
    "     \n",
    "   1. SVM  Lineal\n",
    "   2. Árbol de decisión con criterio Gini\n",
    "   3. SVM con kernel rbf\n",
    "   4. Multinomial Naive Bayes \n",
    "   5. Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = transformed_train\n",
    "y_train = np.asarray(df_train.sentiment.values).ravel().tolist()\n",
    "x_test = transformed_test\n",
    "y_test = np.asarray(df_test.sentiment.values).ravel().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model1 = LinearSVC()\n",
    "model1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "\n",
    "model2= Tree()\n",
    "model2.set_params(criterion='gini',splitter='best')\n",
    "model2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model3 = GaussianNB()\n",
    "model3.fit(x_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model4 = MultinomialNB()\n",
    "model4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "model5=LR()\n",
    "model5.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1_tr, result1_t  = model1.predict(x_train), model1.predict(x_test)\n",
    "result2_tr, result2_t  = model2.predict(x_train), model2.predict(x_test)\n",
    "result3_tr, result3_t  = model3.predict(x_train.toarray()), model3.predict(x_test.toarray())\n",
    "result4_tr, result4_t  = model4.predict(x_train), model4.predict(x_test)\n",
    "result5_tr, result5_t  = model5.predict(x_train), model5.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "miss1_tr = (1-accuracy_score(y_train, result1_tr))\n",
    "miss1_t = (1-accuracy_score(y_test, result1_t))\n",
    "\n",
    "miss2_tr = (1-accuracy_score(y_train, result2_tr))\n",
    "miss2_t = (1-accuracy_score(y_test, result2_t))\n",
    "\n",
    "miss3_tr = (1-accuracy_score(y_train, result3_tr))\n",
    "miss3_t = (1-accuracy_score(y_test, result3_t))\n",
    "\n",
    "miss4_tr = (1-accuracy_score(y_train, result4_tr))\n",
    "miss4_t = (1-accuracy_score(y_test, result4_t))\n",
    "\n",
    "miss5_tr = (1-accuracy_score(y_train, result5_tr))\n",
    "miss5_t = (1-accuracy_score(y_test, result5_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n_groups = 5\n",
    "\n",
    "miss_tr = (miss1_tr, miss2_tr, miss3_tr, miss4_tr, miss5_tr)\n",
    "\n",
    "miss_t = (miss1_t, miss2_t, miss3_t, miss4_t, miss5_t)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, miss_tr, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 error_kw=error_config,\n",
    "                 label='Training')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, miss_t, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='r',\n",
    "                 error_kw=error_config,\n",
    "                 label='Test')\n",
    "\n",
    "plt.xlabel('Clasificadores')\n",
    "plt.ylabel('Missclasification')\n",
    "plt.xticks(index + bar_width / 2, ('Linear SVM', 'Tree', 'NB', 'Multinomial NB', 'LR'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gráfico anterior se puede ver el comportamiento, con el porcentaje de datos mal clasificados de cada clasificador, tanto para el conjunto de entremaiento como de pruebas.\n",
    "\n",
    "Para este caso la Regresión Logística es la que entrega mejores resultados para el conjunto de pruebas, el cual es el que más interesa al ser datos nuevos. Se debe notar también un efecto que ocurre con el clasificador de Árbol de decisión. Este es muy bueno para el conjunto de entrenamiento pero uno de los peores para el conjunto de pruebas. Esto corrobora la teoría que dice que este clasificador sufre de un alto overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el classification report ofrecido por las métricas de sklearn, se debe definir los siguientes conceptos:\n",
    "\n",
    "**Precision:** Este se refiere a que tan acertado estuvo la clasificación de cierta clase. Por ejemplo si predijo que existían 10 tweets positivos, pero al final solo 7 de esos lo eran y los otros tres corresponden a falsos positivos, entonces la precisión será $7/10$.\n",
    "\n",
    "**Recall:** A diferencia de precision, saca un porcentaje con respecto a todo el conjunto de datos. Supongamos nuevamente los 7 tweets predecidos correctamente positivos, pero en todo el conjunto de datos existen 15 tweets positivos. Por lo que el recall será de $7/15$\n",
    "\n",
    "**F1-score:** Es una medida que combina la precisión con el recall. Es el promedio harmonico entre estos dos. Se calcula de la siguiente manera:\n",
    "\n",
    "$$F = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}$$\n",
    "\n",
    "**Support:** Indica la cantidad de elementos por conjunto en los datos reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def score_the_model(model,x,y,xt,yt):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print \"Training Accuracy: %f\"%(acc_tr)\n",
    "    print \"Test Accuracy: %f\"%(acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_the_model(model1, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_the_model(model2, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_the_model(model3, x_train.toarray(), y_train, x_test.toarray(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_the_model(model4, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_the_model(model5, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como se ve en el gráfico de la pregunta anterior cualitativamente, la Regresión Logística tiene el mejor comportamiento. Gracias a las métricas de sklearn esto se puede comprobar cuantitativamente, además se puede notar que es levemente superior a SVM lineal. \n",
    "\n",
    "A pesar de que el Árbol de decisión empeora bastante comparado con el conjunto de entrenamiento, este clasificador sigue siendo mejor que Naive Bayes, el cual para este caso es el que se obtiene peores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se replica el procesamiento anteriormente realizado en el punto c) y en el punto e). Con la diferencia que ahora se tendrán múltiples clases. Se enumeran estas desde el 0 al 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     8638\n",
       "7     8459\n",
       "1     5209\n",
       "8     5165\n",
       "2     3842\n",
       "3     2187\n",
       "4     1776\n",
       "5     1526\n",
       "9     1323\n",
       "10     827\n",
       "6      759\n",
       "11     179\n",
       "12     110\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = ['happiness', 'love', 'surprise', 'fun', 'relief', 'enthusiasm', 'worry', 'sadness', 'hate', 'empty', 'boredom', 'anger']\n",
    "\n",
    "df_mul = df_aux.copy()\n",
    "\n",
    "for i in range(count):\n",
    "    if df_mul.sentiment[i] == 'neutral':\n",
    "        df_mul.sentiment[i] = 0\n",
    "    else:\n",
    "        df_mul.sentiment[i] = 1 + emotions.index(df_mul.sentiment[i])\n",
    "        \n",
    "df_mul.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "msk = np.random.rand(len(df_mul)) < 0.8\n",
    "df_train = df_mul[msk]\n",
    "df_test = df_mul[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(df_train.content)\n",
    "X_test_counts = count_vect.transform(df_test.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = X_train_tfidf\n",
    "y_train = np.asarray(df_train.sentiment.values).ravel().tolist()\n",
    "x_test = X_test_tfidf\n",
    "y_test = np.asarray(df_test.sentiment.values).ravel().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## j)\n",
    "Utilice los clasificadores que son extendidos por defecto a m´ultiples clases para detectar emociones en\n",
    "cada tweet, muestre sus desempe˜nos a trav´es del error de pruebas en un gr´afico resumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    " \n",
    "model11 = KNeighborsClassifier()\n",
    "model11.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model12 = MultinomialNB()\n",
    "model12.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result11_tr, result11_t  = model11.predict(x_train), model11.predict(x_test)\n",
    "result12_tr, result12_t  = model12.predict(x_train), model12.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "miss11_tr = (1-accuracy_score(y_train, result11_tr))\n",
    "miss11_t = (1-accuracy_score(y_test, result11_t))\n",
    "\n",
    "miss12_tr = (1-accuracy_score(y_train, result12_tr))\n",
    "miss12_t = (1-accuracy_score(y_test, result12_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8xPSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1151a8cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n_groups = 2\n",
    "\n",
    "miss_tr = (miss11_tr, miss12_tr)\n",
    "\n",
    "miss_t = (miss11_t, miss12_t)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, miss_tr, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 error_kw=error_config,\n",
    "                 label='Training')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, miss_t, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='r',\n",
    "                 error_kw=error_config,\n",
    "                 label='Test')\n",
    "\n",
    "plt.xlabel('Clasificadores')\n",
    "plt.ylabel('Missclasification')\n",
    "plt.xticks(index + bar_width / 2, ('kNN', 'Multinomial NB'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k)\n",
    "Utilice clasificadores binarios que pueden ser extendidos a trav´es de otras t´ecnicas, tal como One vs\n",
    "One y One vs All/Rest [14]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import SVC as SVM\n",
    "\n",
    "classif21 = OneVsRestClassifier(SVM())\n",
    "classif21.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVM\n",
    "\n",
    "model22 = SVM()\n",
    "model22.set_params(kernel='linear')\n",
    "\n",
    "classif22 = OneVsRestClassifier(model22)\n",
    "classif22.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result21_tr, result21_t  = classif21.predict(x_train), classif21.predict(x_test)\n",
    "result22_tr, result22_t  = classif22.predict(x_train), classif22.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "miss21_tr = (1-accuracy_score(y_train, result21_tr))\n",
    "miss21_t = (1-accuracy_score(y_test, result21_t))\n",
    "\n",
    "miss22_tr = (1-accuracy_score(y_train, result22_tr))\n",
    "miss22_t = (1-accuracy_score(y_test, result22_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "model31 = LR()\n",
    "model31.set_params(multi_class = 'ovr')\n",
    "model31.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "model32 = LR()\n",
    "model32.set_params(multi_class = 'multinomial', solver = 'newton-cg')\n",
    "model32.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result31_tr, result31_t  = model31.predict(x_train), model31.predict(x_test)\n",
    "result32_tr, result32_t  = model32.predict(x_train), model32.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "miss31_tr = (1-accuracy_score(y_train, result31_tr))\n",
    "miss31_t = (1-accuracy_score(y_test, result31_t))\n",
    "\n",
    "miss32_tr = (1-accuracy_score(y_train, result32_tr))\n",
    "miss32_t = (1-accuracy_score(y_test, result32_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss31_tr, miss31_t, miss32_tr, miss32_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n_groups = 6\n",
    "\n",
    "miss_tr = (miss11_tr, miss12_tr, miss21_tr, miss22_tr, miss31_tr, miss32_tr)\n",
    "\n",
    "miss_t = (miss11_t, miss12_t, miss21_t, miss22_t, miss31_t, miss32_t)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, miss_tr, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 error_kw=error_config,\n",
    "                 label='Training')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, miss_t, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='r',\n",
    "                 error_kw=error_config,\n",
    "                 label='Test')\n",
    "\n",
    "plt.xlabel('Clasificadores')\n",
    "plt.ylabel('Missclasification')\n",
    "plt.xticks(index + bar_width / 2, ('kNN', 'Multinomial NB', 'LR ovo', 'SVM Linear', 'LR ovr', 'Multinomial LR'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
