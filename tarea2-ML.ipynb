{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2 - Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claudia Hazard 201404523-9\n",
    "## Matías Araya 201173082-8\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tipos de fronteras en Clasificación\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Se comienza creando dataset con 2 dimensiones, conformado por dos conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples=500\n",
    "mean = (0,-4)\n",
    "C = np.array([[0.3, 0.1], [0.1, 1.5]])\n",
    "datos1 = np.random.multivariate_normal(mean, C, n_samples)\n",
    "outer_circ_x = np.cos(np.linspace(0, np.pi, n_samples))*3\n",
    "outer_circ_y = np.sin(np.linspace(0, np.pi, n_samples))*3\n",
    "datos2 = np.vstack((outer_circ_x,outer_circ_y)).T\n",
    "from sklearn.utils import check_random_state\n",
    "generator = check_random_state(10)\n",
    "datos2 += generator.normal(scale=0.3, size=datos2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrega ruido al conjunto de datos para así realizar un estudio mas realista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((datos1, datos2), axis=0)\n",
    "print len(X)\n",
    "n = 20 #ruido/noise\n",
    "y1 = np.zeros(datos1.shape[0]+n)\n",
    "y2 = np.ones(datos2.shape[0]-n)\n",
    "y = np.concatenate((y1,y2),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la configuración de el código anterior existen $1000$ datos en total, los cuales $520$ corresponden a un grupo (puntos azules) y $480$ a otro (puntos verdes). Se nota como el ruido de de $20$ correspondientes realmente al grupo azul, se asemejan más a la figura del grupo verde por lo que genera ruido a la muestra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `visualize_border` es de utilidad para visualizar el conjunto de datos con su respectivo clasificador, el que se utilizará en preguntas posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_border(model,x,y,title=\"\"):\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "    plt.scatter(x[:,0], x[:,1], s=50, c=y, cmap=plt.cm.winter)\n",
    "    h = .02 # step size in the mesh\n",
    "    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) LDA\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "model_lda = LDA()\n",
    "model_lda.fit(X,y)\n",
    "visualize_border(model_lda,X,y,\"LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Linear Discriminant Analysis (LDA), como se ve en la figura mostrada, traza una linea clara que logra separar la clasificación de ambos grupos, quedando así la mayoría de los puntos azules por un lado y la totalidad de los verdes en el otro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) QDA\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "model_qda = QDA()\n",
    "model_qda.fit(X,y)\n",
    "visualize_border(model_qda,X,y,\"QDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso con Quadratic Discriminant Analysis (QDA), como lo dice su nombre al ser de tipo cuadrático, logra crear una curva asemejandose de mejor manera a la figura y cualitativamente se podría decir que clasifica mejor que LDA. Esto sin embargo conlleva un mayor costo de computación además de mayor posibilidad de overfitting, con lo que se analiza en próxima sección si vale la pena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(datos1, bins='auto')\n",
    "plt.title(\"Histograma datos 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(datos2, bins='auto')\n",
    "plt.title(\"Histograma datos 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_lda = model_lda.predict(X)\n",
    "y_pred_qda = model_qda.predict(X)\n",
    "\n",
    "y_true = y\n",
    "\n",
    "print(\"Miss Classification Loss LDA: %f\"%(1-accuracy_score(y_true, y_pred_lda)))\n",
    "print(\"Miss Classification Loss QDA: %f\"%(1-accuracy_score(y_true, y_pred_qda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto se puede ver que la diferencia entre los errores es muy pequña, donde LDA tiene un error de clasificación de $0.021$ y QDA es levemente menor con $0.020$. Con esto se puede comprobar, para este pequeño caso, que no es necesario utilizar QDA ya que genera un gasto extra además de generar un mayor overfitting y no se obtiene una mejor calidad en la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "\n",
    "def visualize_border_interactive(param):\n",
    "    model = train_model(param)\n",
    "    visualize_border(model,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "def train_model(param):\n",
    "    model=LR() #define your model\n",
    "    model.set_params(C=param,penalty='l2')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 0.1\n",
    "p_max = 10\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar en el gráfico interactivo, cambiando el paramétro se mueve ligeramente la línea divisiora de la regresión que separa ambas clases. Esto se puede asemejar a lo que hace Ridge o Lasso, donde se penalizan los coeficientes restando importancia a los que influyen de menor manera en el modelo. Así, si el parámetro es bajo, cercano a cero, tiene menos aceptación a que haya puntos mal clasificados. Con el parámetro en $0.1$ ni un punto verde queda completamente en el grupo azul. Mientras que, con un parámetro más alto, por ejemplo $10$ se nota como cambia donde la pendiente de la recta se hace más pronunciada, aceptando así algunos puntos verdes en el grupo azul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM), a diferencia de la otras técnicas que se han visto, busca minimizar la distancia a los puntos más cercanos del hiperplano. No como el resto de los métodos que minimiza los errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "\n",
    "def train_model(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='linear')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 0.1\n",
    "p_max = 1\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El efecto es casi el mismo que la Regresión Logística, donde con una valor alto de el parámetro $C$ la línea divisora queda con una mayor pendiente, aceptando así los valores azules correspondientes al ruido pero incluyendo también unos pocos del conjunto verde. Mientras que con un $C$ cercano a cero permite menos puntos azules de los correspondientes al ruido pero no acepta ni un verde en el conjunto azul. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una solución simplificada para SVM no Lineal puede ser escrita como:\n",
    "\n",
    "$$\\hat{f(x)} = \\displaystyle\\sum_{i=1}^{n} \\hat{\\alpha_i}y_i K(x,x_i) + \\hat{\\beta_0}$$\n",
    "\n",
    "Donde $K(x, x_i)$ es la función Kernel y se cumple que para todo $x_i$, $0 < \\alpha_i < C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='rbf')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 0.1\n",
    "p_max = 1\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='poly')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 0.1\n",
    "p_max = 1\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rol del parámetro $C$ es claro en un espacio con más atributos, esto dado que una separación perfecta es usualmente conseguible. Un valor grande de $C$ hará que sea más ondulado, mientras que uno más pequeño reflejará curvas más suaves.\n",
    "\n",
    "El efecto anterior explicado se puede ver de buena manera con el Kernel rbf, donde cambiando el parámetro con un valor de $1$ quedan las líneas más onduladas encerrando al conjunto verde. Mientras cuando se asigna un valor cercano a cero, $0.1$, se ve reflejado que las línas encierran al grupo azul siendo más suave las curvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## h)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "def train_model(param):\n",
    "    model= Tree() #edit the train_model function\n",
    "    model.set_params(max_depth=param,criterion='gini',splitter='best')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 1\n",
    "p_max = 4\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro que se varía es el de maxima profundidad del árbol. Primero se realiza el ánalisis con el criterio Gini, el cual es la suma de las varianzas de cada distribución. Con el parámetro igual a $1$, solo se tendrá un árbol de profundidad 1, por lo que, como se ve en la imagen, separa el conjunto de datos en $2$. Separando claramente ambos conjuntos de datos (azules y verdes).\n",
    "\n",
    "Si esta parámetro aumenta, se agrega un nivel de decisión y de profundidad, por lo que se vuelve más preciso el árbol. Como se puede ver en el código anterior, con 3 niveles es capaz de capturar gran parte del ruido en el conjunto verde a su correspondiente conjunto.\n",
    "\n",
    "La gran ventaja de este método es su simpleza para ser interpretado, pero sufre de un gran overfitting comparado con los otros métodos anteriormente vistos. Esto se puede comprobar que si el parámetro es $4$, se agrega una tercera clase inexistente con puntos azules y verdes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "def train_model(param):\n",
    "    model= Tree() #edit the train_model function\n",
    "    model.set_params(max_depth=param,criterion='entropy',splitter='best')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 2\n",
    "p_max = 8\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cambia el criterio a information gain, este intenta minimizar la suma pesada de las entropías resultantes. Este es equivalente a maximizar la ganancia de información al dividir el nodo. En otras palabras se debe buscar una medida de la pureza de cada nodo resultante.\n",
    "\n",
    "Con poca entropía es claro saber a que clase corresponde, si es muy entrópica no se tiene esa claridad.\n",
    "\n",
    "Con este criterio es notorio que se obtienen mejores resultado cuando el parámetro es alto en comparación a Gini, donde por ejemplo con $8$ logra diferenciar los puntos azules del ruido sin agregar puntos verdes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4599b57bbef14bb8b635eb4294771fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def train_model(param):\n",
    "    model = KNeighborsClassifier()\n",
    "    model.set_params(n_neighbors=param)\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "p_min = 1\n",
    "p_max = 10\n",
    "interactive(visualize_border_interactive, param=(p_min, p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo kNN se refiere a k Nearest Neighbors, por lo que el parámetro $k$ es para ver los vecinos más cercanos. Si se elige por ejemplo $k=2$, un punto quedará en el conjunto donde sus dos vecinos más cercanos estén. Esto se puede ver cualitativamente en el gráfico, donde con un $k$ bajo en la sección donde está el ruido las líneas se dispersan de manera que encierran de a uno los puntos. Mientras que con un $k$ mayor la línea se hace más continua."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análisis de audios como datos brutos\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "def clean_filename(fname, string):\n",
    "    file_name = fname.split('/')[1]\n",
    "    if file_name[:2] == '__':\n",
    "        file_name = string + file_name\n",
    "    return file_name\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "def load_wav_file(name, path):\n",
    "    s, b = wavfile.read(path + name)\n",
    "    assert s == SAMPLE_RATE\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)\n",
    "Se crea el dataset a partir de  set_a.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./latidos/set_a.csv')\n",
    "df.info()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se calculan la cantidad de registros por clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art=0\n",
    "mur=0\n",
    "extra=0\n",
    "norm=0\n",
    "nulo=0\n",
    "for i in df[\"label\"]:\n",
    "    if i==\"artifact\":\n",
    "        art+=1\n",
    "    elif i==\"murmur\":\n",
    "        mur+=1\n",
    "    elif i==\"extrahls\":\n",
    "        extra+=1\n",
    "    elif i==\"normal\":\n",
    "        norm+=1\n",
    "    else:\n",
    "        nulo+=1\n",
    "print (\"Tipo artifact:\"+str(art))\n",
    "print (\"Tipo extrahls:\"+str(extra))\n",
    "print (\"Tipo murmur:\"+str(mur))\n",
    "print (\"Tipo normal:\"+str(norm))\n",
    "print (\"Tipo no determinado:\"+str(nulo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset esta compuesto por 4 columnas de las cuales dataset y sublabel no aportan mucho pues dataset siempre toma el valor de a y sublabel no tiene ningun dato. En cuanto a las columnas fname y label, fname corresponde a el nombre del archivo que contiene la data, el tipo de sonido y luego el nombre del archivo wav, mientras que label corresponde al tipo de sonido nuevamente.\n",
    "\n",
    "Ademas hay 40 registros del tipo artifact, 19 del tipo extrahls, 34 murmur, 31 normal y 52 sin tipo especifico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "file = csv.reader(open(\"set_a.csv\"))\n",
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "d4=[]\n",
    "for _,fname,label,_ in file:\n",
    "    if \"/\" in fname:\n",
    "        a =clean_filename(fname,\"\")\n",
    "        tipo=a.split(\"__\")[0]\n",
    "        nom=a.split(\"__\")[1]\n",
    "        dat=load_wav_file(nom,\"./latidos/\")\n",
    "        if tipo==\"normal\":\n",
    "            d1.extend(dat)\n",
    "        if tipo==\"murmur\":\n",
    "            d2.extend(dat)\n",
    "        if tipo==\"artifact\":\n",
    "            d3.extend(dat)\n",
    "        if tipo==\"extrahls\":\n",
    "            d4.extend(dat)        \n",
    "plt.hist([d1, d2, d3, d4],color=[\"r\",\"y\",\"b\",\"g\"],label=[\"Normal\",\"Murmur\",\"Artifact\",\"Extra Heart Sound\"],alpha=0.7)\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "\n",
    "Se leen los archivos .wav y se transforman en secuencias de tiempo, es decir, la amplitud del audio espaciadas de manera uniforme en el tiempo. Luego se realiza un padding de ceros al final de cada secuencia para que todas las secuencias de tiempo queden con el mismo largo o misma cantidad de mediciones. Es importante realizar esto pues para realizar el aprendizaje de maquina mas adelante se realizará una transformada de fourier pasando las amplitudes a frecuencias, al realizar el zero padding todos los registros estarán en el mismo espectro de frecuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def padd_zeros(array,length):\n",
    "    aux = np.zeros(length)\n",
    "    aux[:array.shape[0]] = array\n",
    "    return aux\n",
    "new_df =pd.DataFrame({'file_name' : df['fname'].apply(clean_filename,string='Aunlabelledtest')})\n",
    "new_df['time_series'] = new_df['file_name'].apply(load_wav_file, path='./latidos/')\n",
    "new_df['len_series'] = new_df['time_series'].apply(len)\n",
    "new_df['time_series']=new_df['time_series'].apply(padd_zeros,length=max(new_df['len_series']))\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario realizar un padding para tener la misma cantidad de elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n",
    "Se cambian las etiquetas de los audios por otras asignadas por un doctor experto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_labels =[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2,\n",
    "2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
    "1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0,\n",
    "2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0,\n",
    "0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "labels = ['artifact','normal/extrahls', 'murmur']\n",
    "new_df['target'] = [labels[i] for i in new_labels]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art=0\n",
    "mur=0\n",
    "norm=0\n",
    "\n",
    "for i in new_df[\"target\"]:\n",
    "    if i==\"artifact\":\n",
    "        art+=1\n",
    "    elif i==\"murmur\":\n",
    "        mur+=1\n",
    "        extra+=1\n",
    "    elif i==\"normal/extrahls\":\n",
    "        norm+=1\n",
    "\n",
    "print (\"Tipo artifact:\"+str(art))\n",
    "print (\"Tipo murmur:\"+str(mur))\n",
    "print (\"Tipo normal/extrahls:\"+str(norm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al realizar los cambios existen solo 3 clases. Artifact con 58 registros, murmur con 53 y normal/extrahls con 65. Al tener etiquetas mal asignadas con los datos se entrenará a la maquina erroneamente, es decir, la función creada por la maquina podrá entregar con muchos mas fallos los valores para un dato de entrada.\n",
    "Si se cambiara un solo dato esta problematica dependerá de la forma en que se esté entrenando la maquina pues hay técnicas que son más y otras menos sensibles a outliers.\n",
    "\n",
    "\n",
    "\n",
    "depende del tipode maquina que se esta utilizando sbm optimizacion de frontera soft margin es distinto, arbol recubridor tampoco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)\n",
    "Se codifican las clases con valores numericos, el valor 0 corresponde a artifact, 1 a murmur y 2 a normal/extrahls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"target\"] = new_df[\"target\"].astype('category')\n",
    "cat_columns = new_df.select_dtypes(['category']).columns\n",
    "new_df[cat_columns] = new_df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)\n",
    "Se desordenan los datos para que no se encuentren ordenados por las etiquetas y se crea una matriz de amplitudes en el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.sample(frac=1,random_state=44)\n",
    "X = np.stack(new_df['time_series'].values, axis=0)\n",
    "y = new_df.target.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que las dimensiones son 176x396900 estas podrían generar problemas puesto que ?????? de que tipo ? dimensionalidad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f)\n",
    "Se realiza la transformada de fourier discreta para pasar los datos desde el dominio de tiempos al dominio de frecuencias en la señal de sonido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_fourier = np.abs(np.fft.fft(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g)\n",
    "Se realiza un muetreo a través de una técnica de muestreo especializada en secuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "X_resampled = []\n",
    "for i in range(X_fourier.shape[0]):\n",
    "    sequence = X_fourier[i,:].copy()\n",
    "    resampled_sequence = signal.resample(sequence, 100000)\n",
    "    X_resampled.append(resampled_sequence)\n",
    "X_resampled = np.array(X_resampled)\n",
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso se realiza un beneficio pues BLABLA, se puede demostrar si el muestreo es representativo ya que."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h)\n",
    "Se genera un conjunto de pruebas mediante hold-out validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i)\n",
    "Se estandarizan los datos para ser trabajados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train)\n",
    "X_test = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## j)\n",
    "Se realiza una reducción de dimensionalidad utilizando PCA, representando los datos en 2 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "d=2\n",
    "pca_model = PCA(n_components=d)\n",
    "pca_model.fit(X_train)\n",
    "X_pca_train = pca_model.transform(X_train)\n",
    "X_pca_test = pca_model.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se visualizan en 2 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "h1=0\n",
    "h2=0\n",
    "h3=0\n",
    "for i in X_pca_train:\n",
    "    if y_train[c]==0:\n",
    "        if h1==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"D\", c=\"y\", alpha=0.7,label=\"artifact\")\n",
    "            h1=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"D\", c=\"y\", alpha=0.7)\n",
    "    if y_train[c]==1:\n",
    "        if h2==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"^\", c=\"b\", alpha=0.7,label=\"normal/extrahls\")\n",
    "            h2=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"^\", c=\"b\", alpha=0.7)\n",
    "    if y_train[c]==2:\n",
    "        if h3==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"+\", c=\"r\",alpha=0.7,label=\"murmur\")\n",
    "            h3=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"+\", c=\"r\",alpha=0.7)\n",
    "    c+=1\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k)\n",
    "Se entrena un modelo de regresión logistica variando el parametro C como 0.0001,0.01,0.1,1,10,100 y 1000. Mostrando un gráfico resumen del error en función del hiper-parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [0.0001,0.01,0.1,1,10,100,1000]\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "def train_model1(param):\n",
    "    model=LR() #define your model\n",
    "    model.set_params(C=param,penalty='l2')\n",
    "    model.fit(X_pca_train,y_train)\n",
    "    return model\n",
    "\n",
    "def grafica1():\n",
    "    e=[]\n",
    "    for n in Cs:\n",
    "        model=train_model1(n)\n",
    "        y_pred = model.predict(X_pca_train)\n",
    "        error = 1-accuracy_score(y_train,y_pred)\n",
    "        e.append(error)\n",
    "    plt.plot(Cs,e, 'r', zorder=1, lw=2)\n",
    "    plt.scatter(Cs,e,marker=\"D\", c=\"g\", alpha=0.7,label=\"Promedio de error:\"+str(sum(e)/len(e)))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "print (grafica1())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena una máquina de soporte vectorial(SVM) con kernel lineal variando el hiper-parámetro C al igual que en la parte anterior y construyendo un gráfico resumen del error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "\n",
    "def train_model2(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='linear')\n",
    "    model.fit(X_pca_train,y_train)\n",
    "    return model\n",
    "def grafica2():\n",
    "    e=[]\n",
    "    for n in Cs:\n",
    "        model=train_model2(n)\n",
    "        y_pred = model.predict(X_pca_train)\n",
    "        error = 1-accuracy_score(y_train,y_pred)\n",
    "        e.append(error)\n",
    "    plt.plot(Cs,e, 'r', zorder=1, lw=2)\n",
    "    plt.scatter(Cs,e,marker=\"D\", c=\"g\", alpha=0.7,label=\"Promedio de error:\"+str(sum(e)/len(e)))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "print (grafica2())\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l)\n",
    "Entrene un Arbol de Decisi´on, con la configuraci´on que estime conveniente, variando el hiper-par´ametro ´\n",
    "regularizador max depth, construyendo un gr´afico resumen del error en funci´on de este par´ametro.\n",
    "Compare con los modelos anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Depths = range(1,30)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "def train_model3(param):\n",
    "    model= Tree() #edit the train_model function\n",
    "    model.set_params(max_depth=param,criterion='entropy',splitter='best')\n",
    "    model.fit(X_pca_train,y_train)\n",
    "    return model\n",
    "def grafica3():\n",
    "    e=[]\n",
    "    for n in Depths:\n",
    "        model=train_model3(n)\n",
    "        y_pred = model.predict(X_pca_train)\n",
    "        error = 1-accuracy_score(y_train,y_pred)\n",
    "        e.append(error)\n",
    "    plt.plot(Depths,e, 'r', zorder=1, lw=2)\n",
    "    plt.scatter(Depths,e,marker=\"D\", c=\"g\", alpha=0.7,label=\"Promedio de error:\"+str(sum(e)/len(e)))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "print (grafica3())\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m)\n",
    "Experimente con diferentes dimensiones d para la proyecci´on de PCA con el prop´osito de obtener un\n",
    "modelo con menor error. Construya una tabla o gr´afico resumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "a=range(-3,4)\n",
    "valores=[]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "for i in a:\n",
    "    valores.append(10**i)\n",
    "for d in valores:\n",
    "    model = PCA(n_components=d)\n",
    "    pca_model.fit(X_train)\n",
    "    X_pca_train = pca_model.transform(X_train)\n",
    "    X_pca_test = pca_model.transform(X_test)\n",
    "    # row and column sharing\n",
    "    grafica1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valores=[]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "for i in a:\n",
    "    valores.append(10**i)\n",
    "for d in valores:\n",
    "    model = PCA(n_components=d)\n",
    "    pca_model.fit(X_train)\n",
    "    X_pca_train = pca_model.transform(X_train)\n",
    "    X_pca_test = pca_model.transform(X_test)\n",
    "    # row and column sharing\n",
    "    grafica2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valores=[]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "for i in a:\n",
    "    valores.append(10**i)\n",
    "for d in valores:\n",
    "    model = PCA(n_components=d)\n",
    "    pca_model.fit(X_train)\n",
    "    X_pca_train = pca_model.transform(X_train)\n",
    "    X_pca_test = pca_model.transform(X_test)\n",
    "    # row and column sharing\n",
    "    grafica3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n)\n",
    " Realice otra reducci´on de dimensionalidad ahora a trav´es de la t´ecnica LDA, para representar los datos\n",
    "en d = 2 dimensiones. Recuerde que s´olo se debe ajustar con el conjunto de entrenamiento, si se muestra\n",
    "un warning explique el porqu´e. Visualice apropiadamente la proyecci´on en 2 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "model_lda = LDA(n_components=2)\n",
    "model_lda.fit(X_train,y_train)\n",
    "X_pca_train = model_lda.transform(X_train)\n",
    "X_pca_test = model_lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=0\n",
    "h1=0\n",
    "h2=0\n",
    "h3=0\n",
    "for i in X_lda_train:\n",
    "    if y_train[c]==0:\n",
    "        if h1==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"D\", c=\"y\", alpha=0.7,label=\"artifact\")\n",
    "            h1=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"D\", c=\"y\", alpha=0.7)\n",
    "    if y_train[c]==1:\n",
    "        if h2==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"^\", c=\"b\", alpha=0.7,label=\"normal/extrahls\")\n",
    "            h2=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"^\", c=\"b\", alpha=0.7)\n",
    "    if y_train[c]==2:\n",
    "        if h3==0:\n",
    "            plt.scatter(i[0],i[1],marker=\"+\", c=\"r\",alpha=0.7,label=\"murmur\")\n",
    "            h3=1\n",
    "        else:\n",
    "            plt.scatter(i[0],i[1],marker=\"+\", c=\"r\",alpha=0.7)\n",
    "    c+=1\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o)\n",
    "Con el prop´osito de encontrar el mejor modelo vuelva a realizar el item h) con el i) en el nuevo espacio\n",
    "generado por la representaci´on seg´un las d dimensiones de la proyecci´on LDA. Esta nueva representaci´on\n",
    "¿mejora o empeora el desempe˜no? Explique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y, test_size=0.25, random_state=42)\n",
    "\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train)\n",
    "X_test = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p)\n",
    "Intente mejorar el desempe˜no de los algoritmos ya entrenados. Dise˜ne ahora sus propias cracter´ısticas\n",
    "(feature crafting) a partir de los datos brutos (secuencia de amplitudes), puede inspirarse en otros\n",
    "trabajos [6] [7] si desea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis de emociones en tweets\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('text_emotion.csv')\n",
    "df.drop(['tweet_id','author'],axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se describe el dataset que se utiliza para este actividad. Corresponde a $40.000$ tweets, los que contienen: id, sentimiento asociado, autor y contenido. Sin embargo, en este paso se limpia el dataset del tweet_id y author ya que no son necesarios para el estudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sentiment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se menciona anteriormente, este dataset consta de $40000$ tweets. Los cuales tienen en total 13 distintas clases o sentimientos asociados. El sentimiento más común es **neutral** con una frecuencia de $8638$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta tabla se puede ver las 13 clases del dataset con sus respectivas frecuencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye un conjunto de entrenamiento y otro de pruebas. Esto se realiza a través de una máscara aleatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>tiffanylu know listenin bad habit earlier sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed headach ughhhh waitin call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funer ceremoni gloomi friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>want hang friend soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dannycastillo want trade someon houston ticke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty   tiffanylu know listenin bad habit earlier sta...\n",
       "1     sadness             layin n bed headach ughhhh waitin call\n",
       "2     sadness                       funer ceremoni gloomi friday\n",
       "3  enthusiasm                              want hang friend soon\n",
       "4     neutral   dannycastillo want trade someon houston ticke..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aux = df.copy()\n",
    "\n",
    "import nltk\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "punc = (u'!', u'-', u'_', u'(', u')', u',', u'.', u':', u';', u'\"', u'\\'', u'?', u'#', u'@', u'$', u'^', u'&', u'*', u'+', u'=', u'{', u'}', u'[', u']', u'\\\\', u'|', u'<', u'>', u'/', u'—',u'...')\n",
    "\n",
    "words = stop.union(punc)\n",
    "\n",
    "count = df_aux.count().content\n",
    "\n",
    "# This for takes a long time. It apply stop word removal, lower casing, deleted puntuaction and stemming. \n",
    "\n",
    "for i in range(count):\n",
    "    tweet = \"\"\n",
    "    for j in word_tokenize(df_aux.content[i].decode('utf-8').lower()):\n",
    "        if j not in words:\n",
    "            tweet += \" \" + porter_stemmer.stem(j)\n",
    "\n",
    "    df_aux.content[i] = tweet\n",
    "    \n",
    "df_aux.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el procesamiento de los tweets se aplica lo siguiente gracias a la librería nltk.\n",
    "<ol>\n",
    "<li>Minúscula a todo el texto</li>\n",
    "<li>Se elimina toda puntuación del texto</li>\n",
    "<li>Se eliminan stop words (articulos, pronombres, preposiciones, etc)</li>\n",
    "<li>Stemming, es decir la reducción de todas las palabras a su tronco léxico base</li>\n",
    "</ol>\n",
    "\n",
    "En el último cuadro se puede ver un ejemplo.\n",
    "\n",
    "**Nota:** Para poder utilizar la librería nltk es necesario tener descargadas las \"stopwords\" que se utilizan en el último código. Estas se pueden obtener descomentando la siguiente línea de código: `nltk.download(\"stopwords\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative = ['worry', 'sadness', 'hate', 'empty', 'boredom', 'anger']\n",
    "positive = ['happiness', 'love', 'surprise', 'fun', 'relief', 'enthusiasm']\n",
    "\n",
    "df_binary = df_aux.copy()\n",
    "\n",
    "for i in range(count):\n",
    "    if df_binary.sentiment[i] in positive:\n",
    "        df_binary.sentiment[i] = 1\n",
    "    elif df_binary.sentiment[i] in negative:\n",
    "        df_binary.sentiment[i] = -1\n",
    "        \n",
    "df_binary = df_binary[df_binary.sentiment != 'neutral']\n",
    "df_binary.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer la reducción binaria al problema se considera: **worry, sadness, hate, empty, boredom y anger** como sentimientos negativos. Mientras que los sentimientos **happiness, love, surprise, fun, relief y enthusiasm** quedan como sentiemientos positivos. Así queda una distribución pareja entre ambos, con solo $764$ tweets de diferencia. Por otro lado, los tweets con sentimiento **neutral**, no serán considerados dada su ambiguedad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define nuevamente la mascara con el nuevo conjunto de datos de clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "msk = np.random.rand(len(df_binary)) < 0.8\n",
    "df_train = df_binary[msk]\n",
    "df_test = df_binary[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta sección al tratarse de la clasificación de un trozo de texto, es necesario hacer un trabajo antes para que así se pueda entrenar el conjunto de datos, donde se representa los tweets como vectores de características *features*. Se requiere contar cuantas veces aparacen ciertas palabras, para esto se construye un vocabulario, el cual es construido por la unión de todas las palabras que aparecen en los tweets. Se utiliza las librerías de *sklearn*, *feature extraction in text*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye primero un vector, el cual tiene dos parámetros `min_df` y `max_df`. Estos se refieren a la cantidad mínima y máxima de veces que debe aparacer una palabra en los tweets para que sea considerado en los tweets. En este caso se decide que debe aparacer por lo menos en un $0.1\\%$ y máximo $10\\%$. Esto hace que se eliminen palabras que probablemente no nos interesan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starting with the CountVectorizer/TfidfTransformer approach...\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "cvec = CountVectorizer(min_df=0.0001, max_df=0.1)\n",
    "cvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, se genrea un vocabulario. En el siguiente código se muestra una lista de algunos de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate all the n-grams found in all documents\n",
    "from itertools import islice\n",
    "cvec_train = cvec.fit_transform(df_train.content)\n",
    "list(islice(cvec.vocabulary_.items(), 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El largo de nuestro vocabulario resultante es de $5779$ palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(cvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza el mismo procedimiento para el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvec_test = cvec.transform(df_test.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podría ser de interés, para entender mejor, verificar cuales son las palabras que más ocurrencias tienen. Esto se puede ver en la siguiente tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "occ = np.asarray(cvec_test.sum(axis=0)).ravel().tolist()\n",
    "counts_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences': occ})\n",
    "counts_df.sort_values(by='occurrences', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Tf-idf se puede saber la frecuencia de cada término. Gracias a esto se obtiene el peso que tiene cada palabra en el texto, además serán los arreglos utilizados en los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformed_train = transformer.fit_transform(cvec_train)\n",
    "transformed_test = transformer.transform(cvec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, como información extra, en el siguiente cuadro se puede ver las palabras que tienen un mayor peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.asarray(transformed_train.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los clasificadores que se utilizarán serán:\n",
    "     \n",
    "   1. SVM  Lineal\n",
    "   2. Árbol de decisión con criterio Gini\n",
    "   3. SVM con kernel rbf\n",
    "   4. Multinomial Naive Bayes \n",
    "   5. Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = transformed_train\n",
    "y_train = np.asarray(df_train.sentiment.values).ravel().tolist()\n",
    "x_test = transformed_test\n",
    "y_test = np.asarray(df_test.sentiment.values).ravel().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model1 = LinearSVC()\n",
    "model1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "\n",
    "model2= Tree()\n",
    "model2.set_params(criterion='gini',splitter='best')\n",
    "model2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model3 = GaussianNB()\n",
    "model3.fit(x_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model4 = MultinomialNB()\n",
    "model4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "model5=LR()\n",
    "model5.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1_tr, result1_t  = model1.predict(x_train), model1.predict(x_test)\n",
    "result2_tr, result2_t  = model2.predict(x_train), model2.predict(x_test)\n",
    "result3_tr, result3_t  = model3.predict(x_train.toarray()), model3.predict(x_test.toarray())\n",
    "result4_tr, result4_t  = model4.predict(x_train), model4.predict(x_test)\n",
    "result5_tr, result5_t  = model5.predict(x_train), model5.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "miss1_tr = (1-accuracy_score(y_train, result1_tr))\n",
    "miss1_t = (1-accuracy_score(y_test, result1_t))\n",
    "\n",
    "miss2_tr = (1-accuracy_score(y_train, result2_tr))\n",
    "miss2_t = (1-accuracy_score(y_test, result2_t))\n",
    "\n",
    "miss3_tr = (1-accuracy_score(y_train, result3_tr))\n",
    "miss3_t = (1-accuracy_score(y_test, result3_t))\n",
    "\n",
    "miss4_tr = (1-accuracy_score(y_train, result4_tr))\n",
    "miss4_t = (1-accuracy_score(y_test, result4_t))\n",
    "\n",
    "miss5_tr = (1-accuracy_score(y_train, result5_tr))\n",
    "miss5_t = (1-accuracy_score(y_test, result5_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n_groups = 5\n",
    "\n",
    "miss_tr = (miss1_tr, miss2_tr, miss3_tr, miss4_tr, miss5_tr)\n",
    "\n",
    "miss_t = (miss1_t, miss2_t, miss3_t, miss4_t, miss5_t)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, miss_tr, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 error_kw=error_config,\n",
    "                 label='Training')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, miss_t, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='r',\n",
    "                 error_kw=error_config,\n",
    "                 label='Test')\n",
    "\n",
    "plt.xlabel('Clasificadores')\n",
    "plt.ylabel('Missclasification')\n",
    "plt.xticks(index + bar_width / 2, ('Linear SVM', 'Tree', 'NB', 'Multinomial NB', 'LR'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gráfico anterior se puede ver el comportamiento, con el porcentaje de datos mal clasificados de cada clasificador, tanto para el conjunto de entremaiento como de pruebas.\n",
    "\n",
    "Para este caso la Regresión Logística es la que entrega mejores resultados para el conjunto de pruebas, el cual es el que más interesa al ser datos nuevos. Se debe notar también un efecto que ocurre con el clasificador de Árbol de decisión. Este es muy bueno para el conjunto de entrenamiento pero uno de los peores para el conjunto de pruebas. Esto corrobora la teoría que dice que este clasificador sufre de un alto overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el classification report ofrecido por las métricas de sklearn, se debe definir los siguientes conceptos:\n",
    "\n",
    "**Precision:** Este se refiere a que tan acertado estuvo la clasificación de cierta clase. Por ejemplo si predijo que existían 10 tweets positivos, pero al final solo 7 de esos lo eran y los otros tres corresponden a falsos positivos, entonces la precisión será $7/10$.\n",
    "\n",
    "**Recall:** A diferencia de precision, saca un porcentaje con respecto a todo el conjunto de datos. Supongamos nuevamente los 7 tweets predecidos correctamente positivos, pero en todo el conjunto de datos existen 15 tweets positivos. Por lo que el recall será de $7/15$\n",
    "\n",
    "**F1-score:** Es una medida que combina la precisión con el recall. Es el promedio harmonico entre estos dos. Se calcula de la siguiente manera:\n",
    "\n",
    "$$F = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}$$\n",
    "\n",
    "**Support:** Indica la cantidad de elementos por conjunto en los datos reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def score_the_model(model,x,y,xt,yt):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print \"Training Accuracy: %f\"%(acc_tr)\n",
    "    print \"Test Accuracy: %f\"%(acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_the_model(model1, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_the_model(model2, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_the_model(model3, x_train.toarray(), y_train, x_test.toarray(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_the_model(model4, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_the_model(model5, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como se ve en el gráfico de la pregunta anterior cualitativamente, la Regresión Logística tiene el mejor comportamiento. Gracias a las métricas de sklearn esto se puede comprobar cuantitativamente, además se puede notar que es levemente superior a SVM lineal. \n",
    "\n",
    "A pesar de que el Árbol de decisión empeora bastante comparado con el conjunto de entrenamiento, este clasificador sigue siendo mejor que Naive Bayes, el cual para este caso es el que se obtiene peores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se replica el procesamiento anteriormente realizado en el punto c) y en el punto e). Con la diferencia que ahora se tendrán múltiples clases. Se enumeran estas desde el 0 al 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     8638\n",
       "7     8459\n",
       "1     5209\n",
       "8     5165\n",
       "2     3842\n",
       "3     2187\n",
       "4     1776\n",
       "5     1526\n",
       "9     1323\n",
       "10     827\n",
       "6      759\n",
       "11     179\n",
       "12     110\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = ['happiness', 'love', 'surprise', 'fun', 'relief', 'enthusiasm', 'worry', 'sadness', 'hate', 'empty', 'boredom', 'anger']\n",
    "\n",
    "df_mul = df_aux.copy()\n",
    "\n",
    "for i in range(count):\n",
    "    if df_mul.sentiment[i] == 'neutral':\n",
    "        df_mul.sentiment[i] = 0\n",
    "    else:\n",
    "        df_mul.sentiment[i] = 1 + emotions.index(df_mul.sentiment[i])\n",
    "        \n",
    "df_mul.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "msk = np.random.rand(len(df_mul)) < 0.8\n",
    "df_train = df_mul[msk]\n",
    "df_test = df_mul[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(min_df=0.0001, max_df=0.1)\n",
    "X_train_counts = count_vect.fit_transform(df_train.content)\n",
    "X_test_counts = count_vect.transform(df_test.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = X_train_tfidf\n",
    "y_train = np.asarray(df_train.sentiment.values).ravel().tolist()\n",
    "x_test = X_test_tfidf\n",
    "y_test = np.asarray(df_test.sentiment.values).ravel().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se utilizan los clasificadores que por defecto son extendidos desde clasificación binaria a múltiple. En este caso serán **KNN** y **Multinomial Naive Bayes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    " \n",
    "model11 = KNeighborsClassifier()\n",
    "model11.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model12 = MultinomialNB()\n",
    "model12.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result11_tr, result11_t  = model11.predict(x_train), model11.predict(x_test)\n",
    "result12_tr, result12_t  = model12.predict(x_train), model12.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "miss11_tr = (1-accuracy_score(y_train, result11_tr))\n",
    "miss11_t = (1-accuracy_score(y_test, result11_t))\n",
    "\n",
    "miss12_tr = (1-accuracy_score(y_train, result12_tr))\n",
    "miss12_t = (1-accuracy_score(y_test, result12_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlksOKDMzyyUHlJmZ5dL/B2F1vOzemeRNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1fe2ddd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n_groups = 2\n",
    "\n",
    "miss_tr = (miss11_tr, miss12_tr)\n",
    "\n",
    "miss_t = (miss11_t, miss12_t)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, miss_tr, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 error_kw=error_config,\n",
    "                 label='Training')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, miss_t, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='r',\n",
    "                 error_kw=error_config,\n",
    "                 label='Test')\n",
    "\n",
    "plt.xlabel('Clasificadores')\n",
    "plt.ylabel('Missclasification')\n",
    "plt.xticks(index + bar_width / 2, ('kNN', 'Multinomial NB'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver el error de missclasification de ambos es altísimo, por lo que no son considerados buenas medidas de clasificación, a pesar de Multinomial NB es levemente mejor. La razón de estos pueden ser debidos a mala configuración por parte del experimentador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se utilizan clasificadores que por lo general son exclusivamente binarios. Estos se extienden a través de la técnica **One vs Rest**. Los que se utilizarán son **SVM** con kernel **rbf** y **linear**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import SVC as SVM\n",
    "\n",
    "classif21 = OneVsRestClassifier(SVM())\n",
    "classif21.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result21_tr, result21_t  = classif21.predict(x_train), classif21.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "miss21_tr = (1-accuracy_score(y_train, result21_tr))\n",
    "miss21_t = (1-accuracy_score(y_test, result21_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gracias a la librería sklearn, la regresión logistica puede ser facilmente extendida a multiples clases a través **One vs Rest** y otro definiedo que la variable a predecir se distribuye **Multinomial**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "model31 = LR()\n",
    "model31.set_params(multi_class = 'ovr')\n",
    "model31.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "model32 = LR()\n",
    "model32.set_params(multi_class = 'multinomial', solver = 'newton-cg')\n",
    "model32.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result31_tr, result31_t  = model31.predict(x_train), model31.predict(x_test)\n",
    "result32_tr, result32_t  = model32.predict(x_train), model32.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "miss31_tr = (1-accuracy_score(y_train, result31_tr))\n",
    "miss31_t = (1-accuracy_score(y_test, result31_t))\n",
    "\n",
    "miss32_tr = (1-accuracy_score(y_train, result32_tr))\n",
    "miss32_t = (1-accuracy_score(y_test, result32_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1f16e190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n_groups = 5\n",
    "\n",
    "miss_tr = (miss11_tr, miss12_tr, miss21_tr, miss31_tr, miss32_tr)\n",
    "\n",
    "miss_t = (miss11_t, miss12_t, miss21_t, miss31_t, miss32_t)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, miss_tr, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 error_kw=error_config,\n",
    "                 label='Training')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, miss_t, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='r',\n",
    "                 error_kw=error_config,\n",
    "                 label='Test')\n",
    "\n",
    "plt.xlabel('Clasificadores')\n",
    "plt.ylabel('Missclasification')\n",
    "plt.xticks(index + bar_width / 2, ('kNN', 'Multinomial NB', 'SVM rbf', 'LR ovr', 'Multinomial LR'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en el gráfico, todos los clasificadores tienen un mal comportamiento y no son suficientes como para poder clasificar los datos de una manera que se pueda predecir. Esto probablemente se deba a una mal preprocesamiento, lo que genera que estén mal pesados luego cuando se realiza la obtención de los features y conlleve a una mala clasificación cuando son múltiples clases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
